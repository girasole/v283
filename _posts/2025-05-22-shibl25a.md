---
title: Scalable Natural Policy Gradient for General-Sum Linear Quadratic Games with
  Known Parameters
abstract: Consider a general-sum N-player linear-quadratic (LQ) game with stochastic
  dynamics over a finite time horizon. It is known that under some mild assumptions,
  the Nash equilibrium (NE) strategies for the players can be obtained by a natural
  policy gradient algorithm. However, the traditional implementation of the algorithm
  requires the availability of complete state and action information from all agents
  and may not scale well with the number of agents. Under the assumption of known
  problem parameters, we present an algorithm that assumes state and action information
  from only neighboring agents according to the graph describing the dynamic or cost
  coupling among the agents. We show that the proposed algorithm converges to an $\epsilon$-neighborhood
  of the NE where the value of $\epsilon$ depends on the size of the local neighborhood
  of agents.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shibl25a
month: 0
tex_title: Scalable Natural Policy Gradient for General-Sum Linear Quadratic Games
  with Known Parameters
firstpage: 139
lastpage: 152
page: 139-152
order: 139
cycles: false
bibtex_author: Shibl, Mostafa and Suttle, Wesley and Gupta, Vijay
author:
- given: Mostafa
  family: Shibl
- given: Wesley
  family: Suttle
- given: Vijay
  family: Gupta
date: 2025-05-22
address:
container-title: Proceedings of the 7th Annual Learning for Dynamics \& Control Conference
volume: '283'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 22
pdf: https://raw.githubusercontent.com/mlresearch/v283/main/assets/shibl25a/shibl25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

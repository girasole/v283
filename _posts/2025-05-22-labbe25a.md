---
title: Analytical Integral Global Optimization
abstract: Numerical optimization has been the workhorse powering the success of many
  machine learning and artificial intelligence tools over the last decade. However,
  current state-of-the-art algorithms for solving unconstrained non-convex optimization
  problems in high-dimensional spaces, either suffer from the curse of dimensionality
  as they rely on sampling, or get stuck in local minima as they rely on gradient-based
  optimization. We present a new graduated optimization method based on the optimization
  of the integral of the cost function over a region, which is incrementally shrunk
  towards a single point, recovering the original problem. We focus on the optimization
  of polynomial functions, for which the integral over simple regions (e.g. hyperboxes)
  can be computed efficiently. We show that this algorithm is guaranteed to converge
  to the global optimum in the simple case of a scalar decision variable. While this
  theoretical result does not extend to the multi-dimensional case, we empirically
  show that our approach outperforms several state-of-the-art algorithms when tested
  on sparse polynomial functions in dimensions up to 170 decision variables.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: labbe25a
month: 0
tex_title: Analytical Integral Global Optimization
firstpage: 711
lastpage: 722
page: 711-722
order: 711
cycles: false
bibtex_author: Labbe, Sebastien and Prete, Andrea Del
author:
- given: Sebastien
  family: Labbe
- given: Andrea Del
  family: Prete
date: 2025-05-22
address:
container-title: Proceedings of the 7th Annual Learning for Dynamics \& Control Conference
volume: '283'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 22
pdf: https://raw.githubusercontent.com/mlresearch/v283/main/assets/labbe25a/labbe25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

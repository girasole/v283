---
title: A Pontryagin Perspective on Reinforcement Learning
abstract: 'Reinforcement learning has traditionally focused on learning state-dependent
  policies to solve optimal control problems in a closed-loop fashion. In this work,
  we introduce the paradigm of open-loop reinforcement learning where a fixed action
  sequence is learned instead. We present three new algorithms: one robust model-based
  method and two sample-efficient model-free methods. Rather than basing our algorithms
  on Bellman’s equation from dynamic programming, our work builds on Pontryagin’s
  principle from the theory of open-loop optimal control. We provide convergence guarantees
  and evaluate all methods empirically on a pendulum swing-up task, as well as on
  two high-dimensional MuJoCo tasks, significantly outperforming existing baselines.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eberhard25a
month: 0
tex_title: A Pontryagin Perspective on Reinforcement Learning
firstpage: 233
lastpage: 244
page: 233-244
order: 233
cycles: false
bibtex_author: Eberhard, Onno and Vernade, Claire and Muehlebach, Michael
author:
- given: Onno
  family: Eberhard
- given: Claire
  family: Vernade
- given: Michael
  family: Muehlebach
date: 2025-05-22
address:
container-title: Proceedings of the 7th Annual Learning for Dynamics \& Control Conference
volume: '283'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 22
pdf: https://raw.githubusercontent.com/mlresearch/v283/main/assets/eberhard25a/eberhard25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

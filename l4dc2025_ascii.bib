@Proceedings{l4dc2025,
    booktitle = {Proceedings of the 7th Annual Learning for Dynamics \& Control Conference},
    title = {Proceedings of the 7th Annual Learning for Dynamics \& Control Conference},
    name = {7th Annual Learning for Dynamics \& Control Conference},
    shortname = {L4DC},
    year = {2025},
    editor = {Ozay, Necmiye and Balzano, Laura, and Panagou, Dimitra and Abate, Alessandro},
    volume = {283},
    start = {2025-06-04},
    end = {2025-06-06},
    published = {2025-05-22},
    address = {University of Michigan, Ann Arbor, MI, USA},
    conference_url = {https://sites.google.com/umich.edu/l4dc2025/}
}

@InProceedings{ziemann25a,
    title = {State space models, emergence, and ergodicity: How many parameters are needed for stable predictions?},
    author = {Ziemann, Ingvar and Matni, Nikolai and Pappas, George},
    pages = {1--11},
    abstract = {How many parameters are required for a model to execute a given task? It has been argued that large language models, pre-trained via self-supervised learning, exhibit emergent capabilities such as multi-step reasoning as their number of parameters reach a critical scale. In the present work, we explore whether this phenomenon can analogously be replicated in a simple theoretical model. We show that the problem of learning linear dynamical systems---a simple instance of self-supervised learning---exhibits a corresponding phase transition. Namely, for every non-ergodic linear system there exists a critical threshold such that a learner using fewer parameters than said threshold cannot achieve bounded error for large sequence lengths. Put differently, in our model we find that tasks exhibiting substantial long-range correlation require a certain critical number of parameters---a phenomenon akin to emergence. We also investigate the role of the learner's parametrization and consider a simple version of a linear dynamical system with hidden state---an imperfectly observed random walk on the real line. For this situation, we show that there exists no learner using a linear filter which can succesfully learn the random walk unless the filter length exceeds a certain threshold depending on the effective memory length and horizon of the problem.}
}

@InProceedings{ghaffari25a,
    title = {Multi-agent Stochastic Bandits Robust to Adversarial Corruptions},
    author = {Ghaffari, Fatemeh and Wang, Xuchuang and Zuo, Jinhang and Hajiesmaili, Mohammad},
    pages = {12--25},
    abstract = {We study cooperative multi-agent multi-armed bandits with adversarial corruption in a heterogeneous setting, where each agent has access to a subset of the full arm set, and the adversary can corrupt the reward observations for all agents. The objective is to maximize the cumulative total reward of all agents (and not be misled by the adversary). We propose a multi-agent cooperative learning algorithm that is robust to adversarial corruption. For this newly devised algorithm, we demonstrate that an adversary with an unknown corruption budget $C$ only incurs an additive $O((L / L_{\min}) C)$ term to the standard regret of the model in non-corruption settings, where $L$ is the total number of agents, and $L_{\min}$ is the minimum number of agents with mutual access to an arm. As independent side contributions, our algorithm improves the state-of-the-art regret bounds when reducing to both the single-agent and homogeneous multi-agent scenarios, tightening multiplicative $K$ (the number of arms) and $L$ (the number of agents) factors, respectively. Lastly, we conduct numerical simulations to corroborate the superiority of our proposed algorithm.}
}

@InProceedings{ziemann25b,
    title = {A Short Information-Theoretic Analysis of Linear Auto-Regressive Learning},
    author = {Ziemann, Ingvar},
    pages = {26--30},
    abstract = {In this note, we give a short information-theoretic proof of the consistency of the Gaussian maximum likelihood estimator in linear auto-regressive models. Our proof yields nearly optimal non-asymptotic rates for parameter recovery and works without any invocation of stability in the case of finite hypothesis classes.}
}

@InProceedings{yardim25a,
    title = {Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement Learning},
    author = {Yardim, Batuhan and He, Niao},
    pages = {31--44},
    abstract = {Mean-field games (MFG) have become significant tools for solving large-scale multi-agent reinforcement learning problems under symmetry. However, the assumptions of access to a known MFG model (which might not be available for real-world games) and of exact symmetry (real-world scenarios often feature heterogeneity) limit the applicability of MFGs. In this work, we broaden the applicability of MFGs by providing a methodology to extend any finite-player, possibly asymmetric, game to an ``induced MFG''. First, we prove that $N$-player dynamic games can be symmetrized and smoothly extended to the infinite-player continuum via Kirszbraun extensions. Next, we define $\alpha,\beta$-symmetric games, a new class of dynamic games that incorporate approximate permutation invariance. We establish explicit approximation bounds for $\alpha,\beta$-symmetric games, demonstrating that the induced mean-field Nash policy is an approximate Nash of the $N$-player game. We analyze TD learning using sample trajectories of the $N$-player game, permitting learning without using an explicit MFG model or oracle. This is used to show a sample complexity of $\widetilde{\mathcal{O}}(\varepsilon^{-6})$ for $N$-agent monotone extendable games to learn an $\varepsilon$-Nash. Evaluations on benchmarks with thousands of agents support our theory of learning under (approximate) symmetry without explicit MFGs.}
}

@InProceedings{li25a,
    title = {DiffuSolve: Diffusion-based Solver for Non-convex Trajectory Optimization},
    author = {Li, Anjian and Ding, Zihan and Dieng, Adji Bousso and Beeson, Ryne},
    pages = {45--58},
    abstract = {Optimal trajectory design is computationally expensive for nonlinear and high-dimensional dynamical systems. The challenge arises from solving a non-convex optimization problem with multiple local optima, where traditional numerical solvers struggle to find diverse solutions efficiently without appropriate initial guesses. In this paper, we introduce DiffuSolve, a general diffusion model-based solver for non-convex trajectory optimization. An expressive diffusion model is trained on pre-collected locally optimal solutions and efficiently samples initial guesses, which then warm-starts numerical solvers to fine-tune the feasibility and optimality. We also present DiffuSolve+, a novel constrained diffusion model with an additional loss in training that further reduces the problem constraint violations of diffusion samples. Experimental evaluations on three tasks verify the improved robustness, diversity, and a 2x to 11x increase in computational efficiency with our proposed method, which generalizes well to trajectory optimization problems of varying challenges.}
}

@InProceedings{ning25a,
    title = {DKMGP: A Gaussian Process Approach to Multi-Task and Multi-Step Vehicle Dynamics Modeling in Autonomous Racing},
    author = {Ning, Jingyun and Behl, Madhur},
    pages = {59--71},
    abstract = {Autonomous racing is gaining attention for its potential to advance autonomous vehicle technologies. Accurate race car dynamics modeling is essential for capturing and predicting future states like position, orientation, and velocity. However, accurately modeling complex subsystems such as tires and suspension poses significant challenges. In this paper, we introduce the Deep Kernel-based Multi-task Gaussian Process (DKMGP), which leverages the structure of a variational multi-task and multi-step Gaussian process model enhanced with deep kernel learning for vehicle dynamics modeling. Unlike existing single-step methods, DKMGP performs multi-step corrections with an adaptive correction horizon (ACH) algorithm that dynamically adjusts to varying driving conditions. To validate and evaluate the proposed DKMGP method, we compare the model performance with DKL-SKIP and a well-tuned single-track model, using high-speed dynamics data (exceeding 230 km/h) collected from a full-scale Indy race car during the Indy Autonomous Challenge held at the Las Vegas Motor Speedway at CES 2024. The results demonstrate that DKMGP achieves upto 99% prediction accuracy compared to one-step DKL-SKIP, while improving real-time computational efficiency by 1752x. Our results show that DKMGP is a scalable and efficient solution for vehicle dynamics modeling making it suitable for high-speed autonomous racing control.}
}

@InProceedings{moniri25a,
    title = {Asymptotics of Linear Regression with Linearly Dependent Data},
    author = {Moniri, Behrad and Hassani, Hamed},
    pages = {72--85},
    abstract = {In this paper we study the asymptotics of linear regression in settings where the covariates exhibit a linear dependency structure, departing from the standard assumption of independence. We model the covariates as a non-Gaussian stochastic process with spatio-temporal covariance and analyze the performance of ridge regression in the high-dimensional proportional regime, where the number of samples and feature dimensions grow proportionally. A Gaussian universality theorem is proven, demonstrating that the asymptotics are invariant under replacing the non-Gaussian covariates with Gaussian vectors preserving mean and covariance, for which tools from random matrix theory can be used to derive precise characterizations of the estimation error. The estimation error is characterized by a fixed-point equation involving the spectral properties of the spatio-temporal covariance matrices, enabling efficient computation. We then study optimal regularization, overparameterization, and the double descent phenomenon in the context of dependent data. Simulations validate our theoretical predictions, shedding light on how dependencies influence estimation error and the choice of regularization parameters.}
}

@InProceedings{soroka25a,
    title = {Learning Temporal Logic Predicates from Data with Statistical Guarantees},
    author = {Soroka, Emi and Sinha, Rohan and Lall, Sanjay},
    pages = {86--98},
    abstract = {Temporal logic rules are often used in control and robotics to provide structured, human-interpretable descriptions of trajectory data. These rules have numerous applications including safety validation using formal methods, constraining motion planning among autonomous agents, and classifying data. However, existing methods for learning temporal logic predicates from data do not provide assurances about the correctness of the resulting predicate. We present a novel method to learn temporal logic predicates from data with finite-sample correctness guarantees. Our approach leverages expression optimization and conformal prediction to learn predicates that correctly describe future trajectories under mild statistical assumptions. We provide experimental results showing the performance of our approach on a simulated trajectory dataset and perform ablation studies to understand how each component of our algorithm contributes to its performance.}
}

@InProceedings{joshi25a,
    title = {Interacting Particle Systems for Fast Linear Quadratic RL},
    author = {Joshi, Anant and Chang, Heng-Sheng and Taghvaei, Amirhossein and Mehta, Prashant G. and Meyn, Sean P.},
    pages = {99--111},
    abstract = {This paper is concerned with the design of algorithms based on systems of interacting particles to represent, approximate, and learn the optimal control law for reinforcement learning (RL). The primary contribution is that convergence rates are greatly accelerated by the interactions between particles. The focus is on the linear quadratic stochastic optimal control problem for which a complete and novel theory is presented. Apart from the new algorithm, sample complexity bounds are obtained, and it is shown that the mean square error scales as $1/N$ where $N$ is the number of particles. The theoretical results and algorithms are illustrated with numerical experiments and comparisons with other recent approaches, where the faster convergence of the proposed algorithm is numerically demonstrated.}
}

@InProceedings{kim25a,
    title = {Learning Two-agent Motion Planning Strategies from Generalized Nash Equilibrium for Model Predictive Control},
    author = {Kim, Hansung and Zhu, Edward L. and Lim, Chang Seok and Borrelli, Francesco},
    pages = {112--123},
    abstract = {We introduce an Implicit Game-Theoretic MPC (IGT-MPC), a decentralized algorithm for two-agent motion planning that uses a learned value function that predicts the game-theoretic interaction outcomes as the terminal cost-to-go function in a model predictive control (MPC) framework, guiding agents to implicitly account for interactions with other agents and maximize their reward. This approach applies to competitive and cooperative multi-agent motion planning problems which we formulate as constrained dynamic games. Given a constrained dynamic game, we randomly sample initial conditions and solve for the generalized Nash equilibrium (GNE) to generate a dataset of GNE solutions, computing the reward outcome of each game-theoretic interaction from the GNE. The data is used to train a simple neural network to predict the reward outcome, which we use as the terminal cost-to-go function in an MPC scheme. We showcase emerging competitive and coordinated behaviors using IGT-MPC in scenarios such as two-vehicle head-to-head racing and un-signalized intersection navigation. IGT-MPC offers a novel method integrating machine learning and game-theoretic reasoning into model-based decentralized multi-agent motion planning.}
}

@InProceedings{raman25a,
    title = {The Complexity of Sequential Prediction in Dynamical Systems},
    author = {Raman, Vinod and Subedi, Unique and Tewari, Ambuj},
    pages = {124--138},
    abstract = {We study the problem of learning to predict the next state of a dynamical system when the underlying evolution function is unknown. Unlike previous work, we place no parametric assumptions on the dynamical system, and study the problem from a learning theory perspective. We define new combinatorial measures and dimensions and show that they quantify the optimal mistake and regret bounds in the realizable and agnostic settings respectively. By doing so, we find that in the realizable setting, the total number of mistakes can grow according to \emph{any} increasing function of the time horizon $T$. In contrast, we show that in the agnostic setting under the commonly studied notion of Markovian regret, the only possible rates are $\Theta(T)$ and $\tilde{\Theta}(\sqrt{T})$.}
}

@InProceedings{shibl25a,
    title = {Scalable Natural Policy Gradient for General-Sum Linear Quadratic Games with Known Parameters},
    author = {Shibl, Mostafa and Suttle, Wesley and Gupta, Vijay},
    pages = {139--152},
    abstract = {Consider a general-sum N-player linear-quadratic (LQ) game with stochastic dynamics over a finite time horizon. It is known that under some mild assumptions, the Nash equilibrium (NE) strategies for the players can be obtained by a natural policy gradient algorithm. However, the traditional implementation of the algorithm requires the availability of complete state and action information from all agents and may not scale well with the number of agents. Under the assumption of known problem parameters, we present an algorithm that assumes state and action information from only neighboring agents according to the graph describing the dynamic or cost coupling among the agents. We show that the proposed algorithm converges to an 
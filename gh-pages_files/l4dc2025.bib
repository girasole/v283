@Proceedings{l4dc2025,
    booktitle = {Proceedings of the 7th Annual Learning for Dynamics \& Control Conference},
    title = {Proceedings of the 7th Annual Learning for Dynamics \& Control Conference},
    name = {7th Annual Learning for Dynamics \& Control Conference},
    shortname = {L4DC},
    year = {2025},
    editor = {Ozay, Necmiye and Balzano, Laura, and Panagou, Dimitra and Abate, Alessandro},
    volume = {283},
    start = {2025-06-04},
    end = {2025-06-06},
    published = {2025-05-22},
    address = {University of Michigan, Ann Arbor, MI, USA},
    conference_url = {https://sites.google.com/umich.edu/l4dc2025/}
}

@InProceedings{ziemann25a,
    title = {State space models, emergence, and ergodicity: How many parameters are needed for stable predictions?},
    author = {Ziemann, Ingvar and Matni, Nikolai and Pappas, George},
    pages = {1--11},
    abstract = {How many parameters are required for a model to execute a given task? It has been argued that large language models, pre-trained via self-supervised learning, exhibit emergent capabilities such as multi-step reasoning as their number of parameters reach a critical scale. In the present work, we explore whether this phenomenon can analogously be replicated in a simple theoretical model. We show that the problem of learning linear dynamical systems--a simple instance of self-supervised learning--exhibits a corresponding phase transition. Namely, for every non-ergodic linear system there exists a critical threshold such that a learner using fewer parameters than said threshold cannot achieve bounded error for large sequence lengths. Put differently, in our model we find that tasks exhibiting substantial long-range correlation require a certain critical number of parameters--a phenomenon akin to emergence. We also investigate the role of the learner's parametrization and consider a simple version of a linear dynamical system with hidden state---an imperfectly observed random walk on the real line. For this situation, we show that there exists no learner using a linear filter which can successfully learn the random walk unless the filter length exceeds a certain threshold depending on the effective memory length and horizon of the problem.}
}

@InProceedings{ghaffari25a,
    title = {Multi-agent Stochastic Bandits Robust to Adversarial Corruptions},
    author = {Ghaffari, Fatemeh and Wang, Xuchuang and Zuo, Jinhang and Hajiesmaili, Mohammad},
    pages = {12--25},
    abstract = {We study cooperative multi-agent multi-armed bandits with adversarial corruption in a heterogeneous setting, where each agent has access to a subset of the full arm set, and the adversary can corrupt the reward observations for all agents. The objective is to maximize the cumulative total reward of all agents (and not be misled by the adversary). We propose a multi-agent cooperative learning algorithm that is robust to adversarial corruption. For this newly devised algorithm, we demonstrate that an adversary with an unknown corruption budget $C$ only incurs an additive $O((L / L_{\min}) C)$ term to the standard regret of the model in non-corruption settings, where $L$ is the total number of agents, and $L_{\min}$ is the minimum number of agents with mutual access to an arm. As independent side contributions, our algorithm improves the state-of-the-art regret bounds when reducing to both the single-agent and homogeneous multi-agent scenarios, tightening multiplicative $K$ (the number of arms) and $L$ (the number of agents) factors, respectively. Lastly, we conduct numerical simulations to corroborate the superiority of our proposed algorithm.}
}

@InProceedings{ziemann25b,
    title = {A Short Information-Theoretic Analysis of Linear Auto-Regressive Learning},
    author = {Ziemann, Ingvar},
    pages = {26--30},
    abstract = {In this note, we give a short information-theoretic proof of the consistency of the Gaussian maximum likelihood estimator in linear auto-regressive models. Our proof yields nearly optimal non-asymptotic rates for parameter recovery and works without any invocation of stability in the case of finite hypothesis classes.}
}

@InProceedings{yardim25a,
    title = {Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement Learning},
    author = {Yardim, Batuhan and He, Niao},
    pages = {31--44},
    abstract = {Mean-field games (MFG) have become significant tools for solving large-scale multi-agent reinforcement learning problems under symmetry. However, the assumptions of access to a known MFG model (which might not be available for real-world games) and of exact symmetry (real-world scenarios often feature heterogeneity) limit the applicability of MFGs. In this work, we broaden the applicability of MFGs by providing a methodology to extend any finite-player, possibly asymmetric, game to an ``induced MFG''. First, we prove that $N$-player dynamic games can be symmetrized and smoothly extended to the infinite-player continuum via Kirszbraun extensions. Next, we define $\alpha,\beta$-symmetric games, a new class of dynamic games that incorporate approximate permutation invariance. We establish explicit approximation bounds for $\alpha,\beta$-symmetric games, demonstrating that the induced mean-field Nash policy is an approximate Nash of the $N$-player game. We analyze TD learning using sample trajectories of the $N$-player game, permitting learning without using an explicit MFG model or oracle. This is used to show a sample complexity of $\widetilde{\mathcal{O}}(\varepsilon^{-6})$ for $N$-agent monotone extendable games to learn an $\varepsilon$-Nash. Evaluations on benchmarks with thousands of agents support our theory of learning under (approximate) symmetry without explicit MFGs.}
}

@InProceedings{li25a,
    title = {DiffuSolve: Diffusion-based Solver for Non-convex Trajectory Optimization},
    author = {Li, Anjian and Ding, Zihan and Dieng, Adji Bousso and Beeson, Ryne},
    pages = {45--58},
    abstract = {Optimal trajectory design is computationally expensive for nonlinear and high-dimensional dynamical systems. The challenge arises from solving a non-convex optimization problem with multiple local optima, where traditional numerical solvers struggle to find diverse solutions efficiently without appropriate initial guesses. In this paper, we introduce DiffuSolve, a general diffusion model-based solver for non-convex trajectory optimization. An expressive diffusion model is trained on pre-collected locally optimal solutions and efficiently samples initial guesses, which then warm-starts numerical solvers to fine-tune the feasibility and optimality. We also present DiffuSolve+, a novel constrained diffusion model with an additional loss in training that further reduces the problem constraint violations of diffusion samples. Experimental evaluations on three tasks verify the improved robustness, diversity, and a 2x to 11x increase in computational efficiency with our proposed method, which generalizes well to trajectory optimization problems of varying challenges.}
}

@InProceedings{ning25a,
    title = {DKMGP: A Gaussian Process Approach to Multi-Task and Multi-Step Vehicle Dynamics Modeling in Autonomous Racing},
    author = {Ning, Jingyun and Behl, Madhur},
    pages = {59--71},
    abstract = {Autonomous racing is gaining attention for its potential to advance autonomous vehicle technologies. Accurate race car dynamics modeling is essential for capturing and predicting future states like position, orientation, and velocity. However, accurately modeling complex subsystems such as tires and suspension poses significant challenges. In this paper, we introduce the Deep Kernel-based Multi-task Gaussian Process (DKMGP), which leverages the structure of a variational multi-task and multi-step Gaussian process model enhanced with deep kernel learning for vehicle dynamics modeling. Unlike existing single-step methods, DKMGP performs multi-step corrections with an adaptive correction horizon (ACH) algorithm that dynamically adjusts to varying driving conditions. To validate and evaluate the proposed DKMGP method, we compare the model performance with DKL-SKIP and a well-tuned single-track model, using high-speed dynamics data (exceeding 230 km/h) collected from a full-scale Indy race car during the Indy Autonomous Challenge held at the Las Vegas Motor Speedway at CES 2024. The results demonstrate that DKMGP achieves upto 99\% prediction accuracy compared to one-step DKL-SKIP, while improving real-time computational efficiency by 1752x. Our results show that DKMGP is a scalable and efficient solution for vehicle dynamics modeling making it suitable for high-speed autonomous racing control.}
}

@InProceedings{moniri25a,
    title = {Asymptotics of Linear Regression with Linearly Dependent Data},
    author = {Moniri, Behrad and Hassani, Hamed},
    pages = {72--85},
    abstract = {In this paper we study the asymptotics of linear regression in settings where the covariates exhibit a linear dependency structure, departing from the standard assumption of independence. We model the covariates as a non-Gaussian stochastic process with spatio-temporal covariance and analyze the performance of ridge regression in the high-dimensional proportional regime, where the number of samples and feature dimensions grow proportionally. A Gaussian universality theorem is proven, demonstrating that the asymptotics are invariant under replacing the non-Gaussian covariates with Gaussian vectors preserving mean and covariance, for which tools from random matrix theory can be used to derive precise characterizations of the estimation error. The estimation error is characterized by a fixed-point equation involving the spectral properties of the spatio-temporal covariance matrices, enabling efficient computation. We then study optimal regularization, overparameterization, and the double descent phenomenon in the context of dependent data. Simulations validate our theoretical predictions, shedding light on how dependencies influence estimation error and the choice of regularization parameters.}
}

@InProceedings{soroka25a,
    title = {Learning Temporal Logic Predicates from Data with Statistical Guarantees},
    author = {Soroka, Emi and Sinha, Rohan and Lall, Sanjay},
    pages = {86--98},
    abstract = {Temporal logic rules are often used in control and robotics to provide structured, human-interpretable descriptions of trajectory data. These rules have numerous applications including safety validation using formal methods, constraining motion planning among autonomous agents, and classifying data. However, existing methods for learning temporal logic predicates from data do not provide assurances about the correctness of the resulting predicate. We present a novel method to learn temporal logic predicates from data with finite-sample correctness guarantees. Our approach leverages expression optimization and conformal prediction to learn predicates that correctly describe future trajectories under mild statistical assumptions. We provide experimental results showing the performance of our approach on a simulated trajectory dataset and perform ablation studies to understand how each component of our algorithm contributes to its performance.}
}

@InProceedings{joshi25a,
    title = {Interacting Particle Systems for Fast Linear Quadratic RL},
    author = {Joshi, Anant and Chang, Heng-Sheng and Taghvaei, Amirhossein and Mehta, Prashant G. and Meyn, Sean P.},
    pages = {99--111},
    abstract = {This paper is concerned with the design of algorithms based on systems of interacting particles to represent, approximate, and learn the optimal control law for reinforcement learning (RL). The primary contribution is that convergence rates are greatly accelerated by the interactions between particles. The focus is on the linear quadratic stochastic optimal control problem for which a complete and novel theory is presented. Apart from the new algorithm, sample complexity bounds are obtained, and it is shown that the mean square error scales as $1/N$ where $N$ is the number of particles. The theoretical results and algorithms are illustrated with numerical experiments and comparisons with other recent approaches, where the faster convergence of the proposed algorithm is numerically demonstrated.}
}

@InProceedings{kim25a,
    title = {Learning Two-agent Motion Planning Strategies from Generalized Nash Equilibrium for Model Predictive Control},
    author = {Kim, Hansung and Zhu, Edward L. and Lim, Chang Seok and Borrelli, Francesco},
    pages = {112--123},
    abstract = {We introduce an Implicit Game-Theoretic MPC (IGT-MPC), a decentralized algorithm for two-agent motion planning that uses a learned value function that predicts the game-theoretic interaction outcomes as the terminal cost-to-go function in a model predictive control (MPC) framework, guiding agents to implicitly account for interactions with other agents and maximize their reward. This approach applies to competitive and cooperative multi-agent motion planning problems which we formulate as constrained dynamic games. Given a constrained dynamic game, we randomly sample initial conditions and solve for the generalized Nash equilibrium (GNE) to generate a dataset of GNE solutions, computing the reward outcome of each game-theoretic interaction from the GNE. The data is used to train a simple neural network to predict the reward outcome, which we use as the terminal cost-to-go function in an MPC scheme. We showcase emerging competitive and coordinated behaviors using IGT-MPC in scenarios such as two-vehicle head-to-head racing and un-signalized intersection navigation. IGT-MPC offers a novel method integrating machine learning and game-theoretic reasoning into model-based decentralized multi-agent motion planning.}
}

@InProceedings{raman25a,
    title = {The Complexity of Sequential Prediction in Dynamical Systems},
    author = {Raman, Vinod and Subedi, Unique and Tewari, Ambuj},
    pages = {124--138},
    abstract = {We study the problem of learning to predict the next state of a dynamical system when the underlying evolution function is unknown. Unlike previous work, we place no parametric assumptions on the dynamical system, and study the problem from a learning theory perspective. We define new combinatorial measures and dimensions and show that they quantify the optimal mistake and regret bounds in the realizable and agnostic settings respectively. By doing so, we find that in the realizable setting, the total number of mistakes can grow according to \emph{any} increasing function of the time horizon $T$. In contrast, we show that in the agnostic setting under the commonly studied notion of Markovian regret, the only possible rates are $\Theta(T)$ and $\tilde{\Theta}(\sqrt{T})$.}
}

@InProceedings{shibl25a,
    title = {Scalable Natural Policy Gradient for General-Sum Linear Quadratic Games with Known Parameters},
    author = {Shibl, Mostafa and Suttle, Wesley and Gupta, Vijay},
    pages = {139--152},
    abstract = {Consider a general-sum N-player linear-quadratic (LQ) game with stochastic dynamics over a finite time horizon. It is known that under some mild assumptions, the Nash equilibrium (NE) strategies for the players can be obtained by a natural policy gradient algorithm. However, the traditional implementation of the algorithm requires the availability of complete state and action information from all agents and may not scale well with the number of agents. Under the assumption of known problem parameters, we present an algorithm that assumes state and action information from only neighboring agents according to the graph describing the dynamic or cost coupling among the agents. We show that the proposed algorithm converges to an $\epsilon$-neighborhood of the NE where the value of $\epsilon$ depends on the size of the local neighborhood of agents.}
}

@InProceedings{compton25a,
    title = {Learning for Layered Safety-Critical Control with Predictive Control Barrier Functions},
    author = {Compton, William D. and Cohen, Max H. and Ames, Aaron D.},
    pages = {153--165},
    abstract = {Safety filters leveraging control barrier functions (CBFs) are highly effective for enforcing safe behavior on complex systems. It is often easier to synthesize these for Reduced order Models (RoMs), and then track the resulting safe control input on the Full order Model (FoM)---yet gaps between the RoM and FoM can result in safety violations. This paper introduces \emph{predictive CBFs} to address this gap: leveraging rollouts of the FoM to define a predictive robustness term added to the CBF condition. Theoretically, we prove that this guarantees safety in a layered control implementation. Practically, we learn the predictive robustness term through massive parallel simulation with domain randomization. We demonstrate in simulation that this yields safe behavior on the FoM with minimal conservatism, and experimentally realize predictive CBFs on a 3D hopping robot.}
}

@InProceedings{gyorok25a,
    title = {Orthogonal projection-based regularization for efficient model augmentation},
    author = {Gy{\"o}r{\"o}k, Bendeguz Mate and Hoekstra, Jan H. and Kon, Johan and Peni, Tamas and Schoukens, Maarten and Toth, Roland},
    pages = {166--178},
    abstract = {Deep-learning-based nonlinear system identification has shown the ability to produce reliable and highly accurate models in practice. However, these black-box models lack physical interpretability, and a considerable part of the learning effort is often spent on capturing already expected/known behavior of the system, that can be accurately described by first-principles laws of physics. A potential solution is to directly integrate such prior physical knowledge into the model structure, combining the strengths of physics-based modeling and deep-learning-based identification. The most common approach is to use an additive model augmentation structure, where the physics-based and the machine-learning (ML) components are connected in parallel, i.e., additively. However, such models are overparametrized, training them is challenging, potentially causing the physics-based part to lose interpretability. To overcome this challenge, this paper proposes an orthogonal projection-based regularization technique to enhance parameter learning and even model accuracy in learning-based augmentation of nonlinear baseline models.}
}

@InProceedings{bhan25a,
    title = {Neural Operators for Predictor Feedback Control of Nonlinear Delay Systems},
    author = {Bhan, Luke and Qin, Peijia and Krstic, Miroslav and Shi, Yuanyuan},
    pages = {179--193},
    abstract = {Predictor feedback designs are critical for delay-compensating controllers in nonlinear systems. However, these designs are limited in practical applications as predictors cannot be directly implemented, but require numerical approximation schemes, which become computationally prohibitive when system dynamics are expensive to compute. To address this challenge, we recast the predictor design as an operator learning problem, and learn the predictor mapping via a neural operator. We prove the existence of an arbitrarily accurate neural operator approximation of the predictor operator. Under the approximated predictor, we achieve semiglobal practical stability of the closed-loop nonlinear delay system. The estimate is semiglobal in a unique sense --- one can enlarge the set of initial states as desired, though this increases the difficulty of training a neural operator, which appears practically in the stability estimate. Furthermore, our analysis holds for any black-box predictor satisfying the universal approximation error bound. We demonstrate the approach by controlling a 5-link robotic manipulator with different neural operator models, achieving significant speedups compared to classic predictor feedback schemes while maintaining closed-loop stability.}
}

@InProceedings{contreras25a,
    title = {Safe, Out-of-Distribution-Adaptive MPC with Conformalized Neural Network Ensembles},
    author = {Contreras, Jose Leopoldo and Shorinwa, Ola and Schwager, Mac},
    pages = {194--207},
    abstract = {We present SODA-MPC, a Safe, Out-of-Distribution-Adaptive Model Predictive Control algorithm, which uses an ensemble of learned models for prediction, with a runtime monitor to flag unreliable out-of-distribution (OOD) predictions. When an OOD situation is detected, SODA-MPC triggers a safe fallback control strategy based on reachability, yielding a control framework that achieves the high performance of learning-based models while preserving the safety of reachability-based control. We demonstrate the method in the context of an autonomous vehicle, driving among dynamic pedestrians, where SODA-MPC uses a neural network ensemble for pedestrian prediction. We use the maximum singular value of the empirical covariance among the ensemble as the OOD signal for the runtime monitor. We calibrate this signal using conformal prediction to derive an OOD detector with probabilistic guarantees on the false-positive rate, given a user-specified confidence level. During in-distribution operation, the MPC controller avoids collisions with a pedestrian based on the trajectory predicted by the mean of the ensemble. When OOD conditions are detected, the MPC switches to a reachability-based controller to avoid collisions with the reachable set of the pedestrian assuming a maximum pedestrian speed, to guarantee safety under the worst-case actions of the pedestrian. We verify SODA-MPC in extensive autonomous driving simulations in a pedestrian-crossing scenario. Our model ensemble is trained and calibrated with real pedestrian data, showing that our OOD detector obtains the desired accuracy rate within a theoretically-predicted range. We empirically show improved safety and improved task completion compared with two state-of-the-art MPC methods that also use conformal prediction, but without OOD adaptation. Further, we demonstrate the effectiveness of our method with the large-scale multi-agent predictor Trajectron++, using large-scale traffic data from the NuScenes dataset for training and calibration.}
}

@InProceedings{nakashima25a,
    title = {Formation Shape Control using the Gromov-Wasserstein Metric},
    author = {Nakashima, Haruto and Ganguly, Siddhartha and Morimoto, Kohei and Kashima, Kenji},
    pages = {208--220},
    abstract = {This article introduces a formation shape control algorithm, in the optimal control framework, for steering an initial population of agents to a desired configuration via employing the Gromov-Wasserstein distance. The underlying dynamical system is assumed to be a constrained linear system and the objective function is a sum of quadratic control-dependent stage cost and a Gromov-Wasserstein terminal cost. The inclusion of the Gromov-Wasserstein cost transforms the resulting optimal control problem into a well-known NP-hard problem, making it both numerically demanding and difficult to solve with high accuracy. Towards that end, we employ a recent semi-definite relaxation-driven technique to tackle the Gromov-Wasserstein distance. A numerical example is provided to illustrate our results.}
}

@InProceedings{mestres25a,
    title = {Anytime Safe Reinforcement Learning},
    author = {Mestres, Pol and Marzabal, Arnau and Cortes, Jorge},
    pages = {221--232},
    abstract = {This paper considers the problem of solving constrained reinforcement learning problems with anytime guarantees, meaning that the algorithmic solution returns a safe policy regardless of when it is terminated. Drawing inspiration from anytime constrained optimization, we introduce Reinforcement Learning-based Safe Gradient Flow (RL-SGF), an on-policy algorithm which employs estimates of the value functions and their respective gradients associated with the objective and safety constraints for the current policy, and updates the policy parameters by solving a convex quadratically constrained quadratic program. We show that if the estimates are computed with a sufficiently large number of episodes (for which we provide an explicit bound), safe policies are updated to safe policies with a probability higher than a prescribed tolerance. We also show that iterates asymptotically converge to a neighborhood of a KKT point, whose size can be arbitrarily reduced by refining the estimates of the value function and their gradients. We illustrate the performance of RL-SGF in a navigation example.}
}

@InProceedings{eberhard25a,
    title = {A Pontryagin Perspective on Reinforcement Learning},
    author = {Eberhard, Onno and Vernade, Claire and Muehlebach, Michael},
    pages = {233--244},
    abstract = {Reinforcement learning has traditionally focused on learning state-dependent policies to solve optimal control problems in a closed-loop fashion. In this work, we introduce the paradigm of open-loop reinforcement learning where a fixed action sequence is learned instead. We present three new algorithms: one robust model-based method and two sample-efficient model-free methods. Rather than basing our algorithms on Bellman's equation from dynamic programming, our work builds on Pontryagin's principle from the theory of open-loop optimal control. We provide convergence guarantees and evaluate all methods empirically on a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks, significantly outperforming existing baselines.}
}

@InProceedings{wang25a,
    title = {Safe Decision Transformer with Learning-based Constraints},
    author = {Wang, Ruhan and Zhou, Dongruo},
    pages = {245--258},
    abstract = {In the field of safe offline reinforcement learning (RL), the objective is to utilize offline data to train a policy that maximizes long-term rewards while adhering to safety constraints. Recent work, such as the Constrained Decision Transformer (CDT) (Liu et al., 2023b), has utilized the Transformer (Vaswani, 2017) architecture to build a safe RL agent that is capable of dynamically adjusting the balance between safety and task rewards. However, it often lacks the stitching ability to output policies that are better than those existing in the offline dataset, similar to other Transformer-based RL agents like the Decision Transformer (DT) (Chen et al., 2021). We introduce the Constrained Q-learning Decision Transformer (CQDT) to address this issue. At the core of our approach is a novel trajectory relabeling scheme that utilizes learned value functions, with careful consideration of the trade-off between safety and cumulative rewards. Experimental results show that our proposed algorithm outperforms several baselines across a variety of safe offline RL benchmarks.}
}

@InProceedings{madhow25a,
    title = {Rates for Offline Reinforcement Learning with Adaptively Collected Data},
    author = {Madhow, Sunil and Qiao, Dan and Yin, Ming and Wang, Yu-Xiang},
    pages = {259--271},
    abstract = {Developing theoretical guarantees on the sample complexity of offline RL methods is an important step towards making data-hungry RL algorithms practically viable. Such results tend to hinge on unrealistic assumptions about the data distribution --- namely that it comprises a set of i.i.d. trajectories collected by a single logging policy. We propose a relaxation of the i.i.d. setting that allows logging policies to depend adaptively upon previous data. For tabular MDPs, we show that minimax-optimal bounds on the sample complexity of offline policy evaluation (OPE) and offline policy learning (OPL) can be recovered under this adaptive setting, and also derive instance-dependent bounds. Finally, we conduct simulations to empirically analyze the behavior of these estimators under adaptive and non-adaptive data. We find that, even while controlling for logging policies, adaptive data can change the signed behavior of estimation error.}
}

@InProceedings{cercola25a,
    title = {Automating the loop in traffic incident management on highway},
    author = {Cercola, Matteo and Gatti, Nicola and Leyva, Pedro Huertas and Carambia, Benedetto and Formentin, Simone},
    pages = {272--284},
    abstract = {Effective traffic incident management is essential for ensuring safety, minimizing congestion, and reducing response times in emergency situations. Traditional highway incident management relies heavily on radio room operators, who must make rapid, informed decisions in high-stakes environments. This paper proposes an innovative solution to support and enhance these decisions by integrating Large Language Models (LLMs) into a decision-support system for traffic incident management. We introduce two approaches: (1) an LLM + Optimization hybrid that leverages both the flexibility of natural language interaction and the robustness of optimization techniques, and (2) a Full LLM approach that autonomously generates decisions using only LLM capabilities. We tested our solutions using historical event data from Autostrade per l'Italia. Experimental results indicate that while both approaches show promise, the LLM + Optimization solution demonstrates superior reliability, making it particularly suited to critical applications where consistency and accuracy are paramount. This research highlights the potential for LLMs to transform highway incident management by enabling accessible, data-driven decision-making support.}
}

@InProceedings{li25c,
    title = {Continual Learning and Lifting of Koopman Dynamics for Linear Control of Legged Robots},
    author = {Li, Feihan and Abuduweili, Abulikemu and Sun, Yifan and Chen, Rui and Zhao, Weiye and Liu, Changliu},
    pages = {136--148},
    abstract = {The control of legged robots, particularly humanoid and quadruped robots, presents significant challenges due to their high-dimensional and nonlinear dynamics. While linear systems can be effectively controlled using methods like Model Predictive Control (MPC), the control of nonlinear systems remains complex. One promising solution is the Koopman Operator, which approximates nonlinear dynamics with a linear model, enabling the use of proven linear control techniques. However, achieving accurate linearization through data-driven methods is difficult due to issues like approximation error, domain shifts, and the limitations of fixed linear state-space representations. These challenges restrict the scalability of Koopman-based approaches. This paper addresses these challenges by proposing Incremental Koopman algorithm designed to iteratively refine Koopman dynamics for high-dimensional legged robots. The key idea is to progressively expand the dataset and latent space dimension, enabling the learned Koopman dynamics to converge towards accurate approximations of the true system dynamics. Theoretical analysis shows that the linear approximation error of our method converges monotonically. Experimental results demonstrate that our method achieves high control performance on robots like Unitree G1/H1/A1/Go2 and ANYmal D, across various terrains using simple linear MPC controllers. This work is the first to successfully apply linearized Koopman dynamics for locomotion control of high-dimensional legged robots, enabling a scalable model-based control solution.}
}

@InProceedings{sun25a,
    title = {Learn With Imagination: Safe Set Guided State-wise Constrained Policy Optimization},
    author = {Sun, Yifan and Li, Feihan and Zhao, Weiye and Chen, Rui and Wei, Tianhao and Liu, Changliu},
    pages = {298--309},
    abstract = {Deep reinforcement learning (RL) excels in various control tasks, yet the absence of safety guarantees hampers its real-world applicability. In particular, explorations during learning usually results in safety violations, while the RL agent learns from those mistakes. On the other hand, safe control techniques ensure persistent safety satisfaction but demand strong priors on system dynamics, which is usually hard to obtain in practice. To address these problems, we present Safe Set Guided State-wise Constrained Policy Optimization (S-3PO), a pioneering algorithm generating state-wise safe optimal policies with zero training violations, i.e., learning without mistakes. S-3PO first employs a safety-oriented monitor with black-box dynamics to ensure safe exploration. It then enforces an "imaginary" cost for the RL agent to converge to optimal behaviors within safety constraints. S-3PO outperforms existing methods in high-dimensional robotics tasks, managing state-wise constraints with zero training violation. This innovation marks a significant stride towards real-world safe RL deployment.}
}

@InProceedings{troch25a,
    title = {Action-Conditioned Hamiltonian Generative Networks (AC-HGN) for Supervised and Reinforcement Learning},
    author = {Troch, Arne and Mets, Kevin and Mercelis, Siegfried},
    pages = {310--322},
    abstract = {This paper introduces Action-Conditioned Hamiltonian Generative Networks (AC-HGN), a physics-informed neural network architecture which learns Hamiltonian dynamics in environments subject to state-dependent external forces. AC-HGN embeds control inputs of any form into an abstract phase space, extending abstract Hamiltonian dynamics with learned external forces. In a supervised setting, results show that AC-HGN surpasses the prediction accuracy of state-of-the-art Lagrangian Neural Networks when trained on a static dataset. Furthermore, AC-HGN can be readily used as a physics-informed world model in a Model-Based Reinforcement Learning (MBRL) setting by embedding policy actions as external forces. Due to the autoencoder structure of AC-HGN, this marks the first Physics-Informed MBRL algorithm which is not reliant on any domain knowledge and is not limited to specific input modalities. Experimental results demonstrate that AC-HGN achieves competitive sample efficiency and asymptotic performance in simple environments, with minimal degradation in more complex environments, while significantly outperforming an uninformed world model. We conclude that the proposed architecture can accurately and efficiently capture environment dynamics and external forces in a Hamiltonian fashion while requiring no domain-specific knowledge, improving the applicability of physics-informed neural networks in supervised and reinforcement learning settings.}
}

@InProceedings{cummins25b,
    title = {Controlling Participation in Federated Learning with Feedback},
    author = {Cummins, Michael and Er, Guner Dilsad and Muehlebach, Michael},
    pages = {174--186},
    abstract = {We address the problem of client participation in federated learning, where traditional methods typically rely on a random selection of a small subset of clients for each training round. In contrast, we propose FedBack, a deterministic approach that leverages control-theoretic principles to manage client participation in ADMM-based federated learning. FedBack models client participation as a discrete-time dynamical system and employs an integral feedback controller to adjust each client's participation rate individually, based on the client's optimization dynamics. We provide global convergence guarantees for our approach by building on the recent federated learning research. Numerical experiments on federated image classification demonstrate that FedBack achieves up to 50\% improvement in communication and computational efficiency over algorithms that rely on a random selection of clients.}
}

@InProceedings{ott25a,
    title = {Informative Input Design for Dynamic Mode Decomposition},
    author = {Ott, Joshua and Kochenderfer, Mykel and Boyd, Stephen},
    pages = {336--349},
    abstract = {Efficiently estimating system dynamics from data is essential for minimizing data collection costs and improving model performance. This work addresses the challenge of designing future control inputs to maximize information gain, thereby improving the efficiency of the system identification process. We propose an approach that integrates informative input design into the Dynamic Mode Decomposition with control (DMDc) framework, which is well-suited for high-dimensional systems. By formulating an approximate convex optimization problem that minimizes the trace of the estimation error covariance matrix, we are able to efficiently reduce uncertainty in the model parameters while respecting constraints on the system states and control inputs. This method outperforms traditional techniques like Pseudo-Random Binary Sequences (PRBS) and orthogonal multisines, which do not adapt to the current system model. We validate our approach using aircraft and fluid dynamics simulations to demonstrate the practical applicability and effectiveness of our method. Our results show that strategically planning control inputs based on the current model enhances the accuracy of system identification while requiring less data. Furthermore, we provide our implementation and simulation interfaces as an open-source software package, facilitating further research development and use by industry practitioners.}
}

@InProceedings{tretiak25a,
    title = {Physics-Enforced Reservoir Computing for Forecasting Spatiotemporal Systems},
    author = {Tretiak, Dima and Bizyaeva, Anastasia and Kutz, J. Nathan and Brunton, Steven L.},
    pages = {350--364},
    abstract = {We present a new approach for incorporating hard physical constraints into reservoir computing (RC). The goal of this work is to increase the reliability, trustworthiness, and generalizability of RC by guaranteeing adherence to known physical laws, particularly while simulating high dimensional systems such as spatiotemporal fluid flows. A reservoir is commonly implemented as a single-layer recurrent neural network in which only the linear output layer is trained and all other parameters are randomly initialized and fixed. Therefore, training a reservoir only involves solving a least squares problem for the weights of the final layer, posing an excellent opportunity to analytically enforce hard constraints. We show that physical constraints, such as conservation laws and boundary conditions, can be imposed in the training procedure and can be guaranteed to hold for forecasting. We introduce physics enforced reservoir computing (PERC) in which the RC training procedure is augmented with a linear homogeneous constraint defined by a linear operator. We then demonstrate this method by enforcing conserved quantities in the Kuramoto-Sivashinsky system and zero-divergence constraints (mass conservation) in the Kolmogorov flow. In both cases, we enforce these constraints to near machine precision. We provide our code online here: https://github.com/dtretiak/PhysicsEnforcedReservoirComputing/tree/main.}
}

@InProceedings{sharpless25a,
    title = {Linear Supervision for Nonlinear, High-Dimensional Neural Control and Differential Games},
    author = {Sharpless, William and Feng, Zeyuan and Bansal, Somil and Herbert, Sylvia},
    pages = {365--377},
    abstract = {As the dimension of a system increases, traditional methods for control and differential games rapidly become intractable, making the design of safe autonomous agents challenging in complex or team settings. Deep-learning approaches avoid discretization and yield numerous successes in robotics and autonomy, but at a higher dimensional limit, accuracy falls as sampling becomes less efficient. We propose using rapidly generated \textit{linear} solutions to the partial differential equation (PDE) arising in the problem to accelerate and improve learned value functions for guidance in high-dimensional, \textit{nonlinear} problems. We define two programs that combine supervision of the linear solution with a standard PDE loss. We demonstrate that these programs offer improvements in speed and accuracy in both a 50-D differential game problem and a 10-D quadrotor control problem.}
}

@InProceedings{kim25c,
    title = {Approximate Thompson Sampling for Learning Linear Quadratic Regulators with $O(\sqrt{T})$ Regret},
    author = {Kim, Yeoneung and Kim, Gihun and Park, Jiwhan and Yang, Insoon},
    pages = {378--391},
    abstract = {We propose a novel Thompson sampling algorithm that learns linear quadratic regulators (LQR) with a Bayesian regret bound of $O(\sqrt{T})$. Our method leverages Langevin dynamics with a carefully designed preconditioner and incorporates a simple excitation mechanism. We show that the excitation signal drives the minimum eigenvalue of the preconditioner to grow over time, thereby accelerating the approximate posterior sampling process. Furthermore, we establish nontrivial concentration properties of the approximate posteriors generated by our algorithm. These properties enable us to bound the moments of the system state and attain an $O(\sqrt{T})$ regret bound without relying on the restrictive assumptions that are often used in the literature.}
}

@InProceedings{zheng25a,
    title = {Extended Convex Lifting for Policy Optimization of Optimal and Robust Control},
    author = {Zheng, Yang and Pai, Chih-Fan and Tang, Yujie},
    pages = {392--404},
    abstract = {Many optimal and robust control problems are nonconvex and potentially nonsmooth in their policy optimization forms. In this paper, we introduce the Extended Convex Lifting (ECL) framework, which reveals hidden convexity in classical optimal and robust control problems from a modern optimization perspective. Our ECL framework offers a bridge between nonconvex policy optimization and convex reformulations. Despite non-convexity and non-smoothness, the existence of an ECL for policy optimization not only reveals that the policy optimization problem is equivalent to a convex problem, but also certifies a class of first-order non-degenerate stationary points to be globally optimal. We further show that this ECL framework encompasses many benchmark control problems, including LQR, state-feedback and output-feedback H-infinity robust control. We believe that ECL will also be of independent interest for analyzing nonconvex problems beyond control.}
}

@InProceedings{bakker25a,
    title = {TamedPUMA: safe and stable imitation learning with geometric fabrics},
    author = {Bakker, Saray and Perez-Dattari, Rodrigo and Santina, Cosimo Della and B{\"o}hmer, Wendelin and Alonso-Mora, Javier},
    pages = {405--418},
    abstract = {Using the language of dynamical systems, Imitation learning (IL) provides an intuitive and effective way of teaching stable task-space motions to robots with goal convergence. Yet, IL techniques are affected by serious limitations when it comes to ensuring safety and fulfillment of physical constraints. With this work, we solve this challenge via TamedPUMA, an IL algorithm augmented with a recent development in motion generation called geometric fabrics. As both the IL policy and geometric fabrics describe motions as artificial second-order dynamical systems, we propose two variations where IL provides a navigation policy for geometric fabrics. The result is a stable imitation learning strategy within which we can seamlessly blend geometrical constraints like collision avoidance and joint limits. Beyond providing a theoretical analysis, we demonstrate TamedPUMA with simulated and real-world tasks, including a 7-DoF manipulator.}
}

@InProceedings{baddam25a,
    title = {Data-Driven Near-Optimal Control of Nonlinear Systems Over Finite Horizon},
    author = {Baddam, Vasanth Reddy and Eldardiry, Hoda and Boker, Almuatazbellah},
    pages = {419--430},
    abstract = {We employ reinforcement learning to address the problem of two-point boundary optimal control for nonlinear systems over a finite time horizon with unknown model dynamics. By leveraging techniques from singular perturbation theory, we decompose the finite-horizon control problem into two sub-problems, each defined over an infinite horizon. This decomposition eliminates the need to solve the time-varying Hamilton-Jacobi-Bellman (HJB) equation, significantly simplifying the process. Using a policy iteration method enabled by this decomposition, we learn the controller gains for each of the two sub-problems. The overall control strategy is then constructed by combining the solutions of these sub-problems. We demonstrate that the performance of the proposed closed-loop system asymptotically approaches the model-based optimal performance as the time horizon becomes large. Finally, we validate our approach through simulation scenarios, which provide strong support for the claims made in this paper.}
}

@InProceedings{akizhanov25a,
    title = {Learning Feasible Transitions for Efficient Contact Planning},
    author = {Akizhanov, Rikhat and Dhedin, Victor and Khadiv, Majid and Laptev, Ivan},
    pages = {431--442},
    abstract = {In this paper, we propose an efficient contact planner for quadrupedal robots to navigate in extremely constrained environments such as stepping stones. The main difficulty in this setting stems from the mixed nature of the problem, namely discrete search over the steppable patches and continuous trajectory optimization. To speed up the discrete search, we study the properties of the transitions from one contact mode to another. In particular, we propose to learn a dynamic feasibility classifier and a target adjustment network. The former predicts if a contact transition between two contact modes is dynamically feasible. The latter is trained to compensate for misalignment in reaching a desired set of contact locations, due to imperfections of the low-level control. We integrate these learned networks in a Monte Carlo Tree Search (MCTS) contact planner. Our simulation results demonstrate that training these networks with offline data significantly speeds up the online search process and improves its accuracy.}
}

@InProceedings{khalyavin25a,
    title = {Learning Kolmogorov-Arnold Neural Activation Functions by Infinite-Dimensional Optimization},
    author = {Khalyavin, Leon and Moreschini, Alessio and Parisini, Thomas},
    pages = {443--455},
    abstract = {Inspired by the Kolmogorov-Arnold Representation Theorem, Kolmogorov-Arnold Networks (KAN) have recently reshaped the landscape of functional representation in the context of training univariate activation functions, thus achieving remarkable performance gains over traditional Multi-Layer Perceptron (MLPs). However, the parametrization of the activation functions in KANs hinders their scalability and usage in machine-learning applications. In this article, we propose a novel infinite-dimensional optimization framework to learn the activation functions. Our game-changing approach enables the achievement of boundedness, interpretability, and, seamless compatibility with training by backpropagation are all preserved for the network, while also significantly reducing the number of parameters required to represent each edge. Through functional representation examples, our results reveal superior accuracy in tasks such as functional representation, positioning our method as a transformative leap forward in neural network design and optimization.}
}

@InProceedings{arnstrom25a,
    title = {Data-Driven and Stealthy Deactivation of Safety Filters},
    author = {Arnstr{\"o}m, Daniel and Teixeira, Andre M.H.},
    pages = {456--468},
    abstract = {Safety filters ensure that control actions that are executed are always safe, no matter the controller in question. Previous work has proposed a simple and stealthy false-data injection attack for deactivating such safety filters. This attack injects false sensor measurements to bias state estimates toward the interior of a safety region, making the safety filter accept unsafe control actions. The attack does, however, require the adversary to know the dynamics of the system, the safety region used in the safety filter, and the observer gain. In this work we relax these requirements and show how a similar data-injection attack can be performed when the adversary only observes the input and output of the observer that is used by the safety filter, without any a priori knowledge about the system dynamics, safety region, or observer gain. In particular, the adversary uses the observed data to identify a state-space model that describes the observer dynamics, and then approximates a safety region in the identified embedding. We exemplify the data-driven attack on an inverted pendulum, where we show how the attack can make the system leave a safe set, even when a safety filter is supposed to stop this from happening.}
}

@InProceedings{agrawal25a,
    title = {Conditional Kernel Imitation Learning for Continuous State Environments},
    author = {Agrawal, Rishabh and Dahlin, Nathan and Jain, Rahul and Nayyar, Ashutosh},
    pages = {469--483},
    abstract = {Imitation Learning (IL) is an important paradigm within the broader reinforcement learning (RL) methodology. Unlike most of RL, it does not assume availability of reward-feedback. Classical methods such as behavioral cloning and inverse reinforcement learning are highly sensitive to estimation errors, especially in continuous state space problems. Meanwhile, state-of-the-art (SOTA) IL algorithms often require additional online interaction data to be effective. In this paper, we consider the problem of imitation learning in continuous state space environments based solely on observed behavior, without access to transition dynamics information, reward structure, or, most importantly, any additional interactions with the environment. Our approach is based on the Markov balance equation and introduces a novel conditional kernel density estimation-based imitation learning framework. It uses conditional kernel density estimators for transition dynamics and seeks to satisfy a balance equation for the environment. We establish that our estimators satisfy asymptotic consistency and present associated sample complexity analysis. Through a series of numerical experiments on continuous state benchmark environments, we show consistently superior empirical performance over many SOTA IL algorithms. The full paper with the appendix is available at: https://github.com/rishabh-1086/CKIL.}
}

@InProceedings{mei25a,
    title = {Flow matching for stochastic linear control systems},
    author = {Mei, Yuhang and Al-Jarrah, Mohammad and Taghvaei, Amirhossein and Chen, Yongxin},
    pages = {484--496},
    abstract = {This paper addresses the problem of steering an initial probability distribution to a target probability distribution through a deterministic or stochastic linear control system. Our proposed approach is inspired by the flow matching methodology, with the difference that we can only affect the flow through the given control channels. The motivation for the problem comes from applications such as robotic swarms and stochastic thermodynamics, where the state of the system, modeled as a probability distribution, should be steered to a desired target configuration. The feedback control law that achieves the task is characterized as the conditional expectation of the control inputs for the stochastic bridges that respect the given control system dynamics. Explicit forms are derived for Gaussian and mixture of Gaussian settings, and a numerical procedure is presented to approximate the control law in the general setting.}
}

@InProceedings{lagemann25a,
    title = {HydroGym: A Reinforcement Learning Platform for Fluid Dynamics},
    author = {Lagemann, Christian and Paehler, Ludger and Callaham, Jared and Mokbel, Sajeda and Ahnert, Samuel and Lagemann, Kai and Lagemann, Esther and Adams, Nikolaus and Brunton, Steven},
    pages = {497--512},
    abstract = {The modeling and control of fluid flows remain a significant challenge with tremendous potential to advance fields including transportation, energy, and medicine. Effective fluid flow control can lead to drag reduction, enhanced mixing, and noise reduction, among other applications. While reinforcement learning (RL) has shown great success in complex domains, such as robotics and protein folding, its application to flow control is hindered by the lack of standardized platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, a scalable runtime, and state-of-the-art RL algorithms. Our platform includes four validated non-differentiable fluid flow environments and one differentiable environment, all evaluated with a variety of modern RL algorithms. HydroGym's scalable design allows computations to run seamlessly from laptops to high-performance computing resources, providing a standardized interface for implementing new flow environments. HydroGym aims to bridge the gap in flow control research, providing a robust platform to support both non-differentiable and differentiable RL techniques, fostering advancements in scientific machine learning.}
}

@InProceedings{hu25a,
    title = {Safe PDE Boundary Control with Neural Operators},
    author = {Hu, Hanjiang and Liu, Changliu},
    pages = {513--526},
    abstract = {The physical world dynamics are generally governed by underlying partial differential equations (PDEs) with unknown analytical forms in science and engineering problems. Neural network based data-driven approaches have been heavily studied in simulating and solving PDE problems in recent years, but it is still challenging to move forward from understanding to controlling the unknown PDE dynamics. PDE boundary control instantiates a simplified but important problem by only focusing on PDE boundary conditions as the control input and output. However, current model-free PDE controllers cannot ensure that the boundary output satisfies some given user-specified safety constraint. To this end, we propose a safety filtering framework to guarantee the boundary output stays within the safe set for current model-free controllers. Specifically, we first introduce a neural boundary control barrier function (BCBF) to ensure the feasibility of the trajectory-wise constraint satisfaction of boundary output. Based on the neural operator modeling the transfer function from boundary control input to output trajectories, we show that the change in the BCBF depends linearly on the change in input boundary, so quadratic programming-based safety filtering can be done for pre-trained model-free controllers. Extensive experiments under challenging hyperbolic, parabolic and Navier-Stokes PDE dynamics environments validate the plug-and-play effectiveness of the proposed method by achieving better general performance and boundary constraint satisfaction compared to the vanilla and constrained model-free controller baselines. The code is available at https://github.com/intelligent-control-lab/safe-pde-control.}
}

@InProceedings{christianson25a,
    title = {Fast and Reliable $N - k$ Contingency Screening with Input-Convex Neural Networks},
    author = {Christianson, Nicolas and Cui, Wenqi and Low, Steven and Yang, Weiwei and Zhang, Baosen},
    pages = {527--539},
    abstract = {Power system operators must ensure that dispatch decisions remain feasible in case of grid outages or contingencies to prevent cascading failures and ensure reliable operation. However, checking the feasibility of all $N - k$ contingencies -- every possible simultaneous failure of $k$ grid components -- is computationally intractable even for small $k$, requiring system operators to resort to heuristic screening methods. Because of the increase in uncertainty and changes in system behaviors, heuristic lists might not include all relevant contingencies, generating false negatives in which unsafe scenarios are misclassified as safe. In this work, we propose to use input-convex neural networks (ICNNs) for contingency screening. We show that ICNN reliability can be determined by solving a convex optimization problem, and by scaling model weights using this problem as a differentiable optimization layer during training, we can learn an ICNN classifier that is both data-driven and has provably guaranteed reliability. That is, our method can ensure a zero false negative rate. We empirically validate this methodology in a case study on the IEEE 39-bus test network, observing that it yields substantial (10-20x) speedups while having excellent classification accuracy.}
}

@InProceedings{kanakeri25a,
    title = {Outlier-Robust Linear System Identification Under Heavy-Tailed Noise},
    author = {Kanakeri, Vinay and Mitra, Aritra},
    pages = {540--551},
    abstract = {We consider the problem of estimating the state transition matrix of a linear time-invariant (LTI) system, given access to multiple independent trajectories sampled from the system. Several recent papers have conducted a non-asymptotic analysis of this problem, relying crucially on the assumption that the process noise is either Gaussian or sub-Gaussian, i.e., "light-tailed". In sharp contrast, we work under a significantly weaker noise model, assuming nothing more than the existence of the fourth moment of the noise distribution. For this setting, we provide the first set of results demonstrating that one can obtain sample-complexity bounds for linear system identification that are nearly of the same order as under sub-Gaussian noise. To achieve such results, we develop a novel robust system identification algorithm that relies on constructing multiple weakly-concentrated estimators, and then boosting their performance using suitable tools from high-dimensional robust statistics. Interestingly, our analysis reveals how the kurtosis of the noise distribution, a measure of heavy-tailedness, affects the number of trajectories needed to achieve desired estimation error bounds. Finally, we show that our algorithm and analysis technique can be easily extended to account for scenarios where an adversary can arbitrarily corrupt a small fraction of the collected trajectory data. Our work takes the first steps towards building a robust statistical learning theory for control under non-ideal assumptions on the data-generating process.}
}

@InProceedings{wang25d,
    title = {Logarithmic Regret for Nonlinear Control},
    author = {Wang, James and Lee, Bruce and Ziemann, Ingvar and Matni, Nikolai},
    pages = {552--565},
    abstract = {We address the problem of learning to control an unknown nonlinear dynamical system through sequential interactions. Motivated by high-stakes applications in which mistakes can be catastrophic, such as robotics and healthcare, we study situations where it is possible for fast sequential learning to occur. Fast sequential learning is characterized by the ability of the learning agent to incur logarithmic regret relative to a fully-informed baseline. We demonstrate that fast sequential learning is achievable in a diverse class of continuous control problems where the system dynamics depend smoothly on unknown parameters, provided the optimal control policy is persistently exciting. Additionally, we derive a regret bound which grows with the square root of the number of interactions for cases where the optimal policy is not persistently exciting. Our results provide the first regret bounds for controlling nonlinear dynamical systems depending nonlinearly on unknown parameters. We validate the trends our theory predicts in simulation on a simple dynamical system.}
}

@InProceedings{ahmad25a,
    title = {Accelerating Proximal Policy Optimization Learning Using Task Prediction for Solving Environments with Delayed Rewards},
    author = {Ahmad, Ahmad and Kermanshah, Mehdi and Leahy, Kevin and Serlin, Zachary and Siu, Ho Chit and Mann, Makai and Vasile, Cristian-Ioan and Tron, Roberto and Belta, Calin},
    pages = {566--578},
    abstract = {In this paper, we tackle the challenging problem of delayed rewards in reinforcement learning (RL). While Proximal Policy Optimization (PPO) has emerged as a leading Policy Gradient method, its performance can degrade under delayed rewards. We introduce two key enhancements to PPO: a hybrid policy architecture that combines an offline policy (trained on expert demonstrations) with an online PPO policy, and a reward shaping mechanism using Time Window Temporal Logic (TWTL). The hybrid architecture leverages offline data throughout training while maintaining PPO's theoretical guarantees. Building on the monotonic improvement framework of Trust Region Policy Optimization (TRPO), we prove that our approach ensures improvement over both the offline policy and previous iterations, with a bounded performance gap of $(2\varsigma\gamma\alpha^2)/(1-\gamma)^2$, where $\alpha$ is the mixing parameter, $\gamma$ is the discount factor, and $\varsigma$ bounds the expected advantage. Additionally, we prove that our TWTL-based reward shaping preserves the optimal policy of the original problem. TWTL enables formal translation of temporal objectives into immediate feedback signals that guide learning. We demonstrate the effectiveness of our approach through extensive experiments on an inverted pendulum and a lunar lander environments, showing improvements in both learning speed and final performance compared to standard PPO and offline-only approaches.}
}

@InProceedings{morimoto25a,
    title = {Linear System Identification from Snapshot Data by Schrodinger bridge},
    author = {Morimoto, Kohei and Kashima, Kenji},
    pages = {579--590},
    abstract = {This paper proposes a system identification method for linear Gaussian systems from snapshot measurement data using Schrodinger bridges (SB). In many practical applications, such as single-cell RNA sequencing, only snapshot measurements of system state are available, making traditional system identification challenging. Our method employs an EM-like algorithm that alternates between trajectory estimation using SB and system parameter inference from the estimated trajectories. The Gaussian assumption for system states and noise allows us to exploit analytical solutions for the SB computation and parameter updates, enabling efficient computation. We also propose a data-driven method for estimating linear Gaussian SB, where marginal parameters are estimated incorporating dynamic constraints. In numerical simulation, we show our method achieves superior identification accuracy and time efficiency.}
}

@InProceedings{bhar25a,
    title = {Scalability Enhancement and Data-Heterogeneity Awareness in Gradient Tracking based Decentralized Bayesian Learning},
    author = {Bhar, Kinjal and Bai, He and George, Jemin and Busart, Carl},
    pages = {591--605},
    abstract = {This paper proposes a Gradient Tracking Decentralized Unadjusted Langevin Algorithm (GT-DULA) to perform Bayesian learning via MCMC sampling. GT-DULA enhances the scalability of the process when compared with the conventional DULA as it reduces the dependence of the convergence bias on the network size by an order of magnitude for constant gradient step size. GT-DULA uses an estimate of the global gradient as a substitute for local gradients which is shared among neighbors in the network. Our theoretical analysis shows that the proposed GT-DULA successfully tracks the global gradient within a certain neighborhood, which leads to a two-fold benefit. First, the optimal mixing of the gradient estimates leads to a lower bias in convergence. Second, the successful tracking of the global gradient implies robustness towards data heterogeneity which is a major concern in decentralized learning.}
}

@InProceedings{bhargav25a,
    title = {Sensor Scheduling in Intrusion Detection Games with Uncertain Payoffs},
    author = {Bhargav, Jayanth and Sundaram, Shreyas and Ghasemi, Mahsa},
    pages = {606--618},
    abstract = {We study the problem of sensor scheduling for an intrusion detection task. We model this as a 2-player zero-sum game over a graph, where the defender (Player 1) seeks to identify the optimal strategy for scheduling sensor orientations to minimize the probability of missed detection at minimal cost, while the intruder (Player 2) aims to identify the optimal path selection strategy to maximize missed detection probability at minimal cost. The defender's strategy space grows exponentially with the number of sensors, making direct computation of the Nash Equilibrium (NE) strategies computationally expensive. To tackle this, we propose a distributed variant of the Weighted Majority algorithm that exploits the structure of the game's payoff matrix, enabling efficient computation of the NE strategies with provable convergence guarantees. Next, we consider a more challenging scenario where the defender lacks knowledge of the true sensor models and, consequently, the game's payoff matrix. For this setting, we develop online learning algorithms that leverage bandit feedback from sensors to estimate the NE strategies. By building on existing results from perturbation theory and online learning in matrix games, we derive high-probability order-optimal regret bounds for our algorithms. Finally, through simulations, we demonstrate the empirical performance of our proposed algorithms in both known and unknown payoff scenarios.}
}

@InProceedings{panaganti25a,
    title = {Bridging Distributionally Robust Learning and Offline RL: An Approach to Mitigate Distribution Shift and Partial Data Coverage},
    author = {Panaganti, Kishan and Xu, Zaiyan and Kalathil, Dileep and Ghavamzadeh, Mohammad},
    pages = {619--634},
    abstract = {The goal of an offline reinforcement learning (RL) algorithm is to learn the optimal policy using offline data, without access to the environment for online exploration. One of the main challenges in offline RL is the distribution shift which refers to the difference between the state-action visitation distribution of the data generating policy and the learning policy. Many recent works have used the idea of pessimism for developing offline RL algorithms and characterizing their sample complexity under a relatively weak assumption of single policy concentrability. Different from the offline RL literature, the area of distributionally robust learning (DRL) offers a principled framework that uses a minimax formulation to tackle model mismatch between training and testing environments. In this work, we aim to bridge these two areas by showing that the DRL approach can tackle the distributional shift problem in offline RL. In particular, we propose two offline RL algorithms using the DRL framework, for the tabular and linear function approximation settings, and characterize their sample complexity under the single policy concentrability assumption. We also demonstrate the performance of our algorithm through simulation experiments and by comparing it with other state-of-the-art tabular offline RL algorithms.}
}

@InProceedings{tang25b,
    title = {Stochastic Real-Time Deception in Nash Equilibrium Seeking for Games with Quadratic Payoffs},
    author = {Tang, Michael and Krstic, Miroslav and Poveda, Jorge},
    pages = {635--646},
    abstract = {In multi-agent autonomous systems, deception is a fundamental concept which characterizes the exploitation of unbalanced information to mislead victims into choosing oblivious actions. This effectively alters the system's long term behavior, leading to outcomes that may be beneficial to the deceiver but detrimental to victim. We study this phenomenon for a class of model-free Nash equilibrium seeking (NES) where players implement independent stochastic exploration signals to learn the pseudogradient flow. In particular, we show that deceptive players who obtain real- time measurements of other players' stochastic perturbation can incorporate this information into their own NES action update, consequentially steering the overall dynamics to a new operating point that could potentially improve the payoffs of the deceptive players. We consider games with quadratic payoff functions, as this restriction allows us to derive a more explicit formulation of the capabilities of the deceptive players. By leveraging results on multi-input stochastic averaging for dynamical systems, we establish local exponential (in probability) convergence for the proposed deceptive NES dynamics. To illustrate our results, we apply them to a two player quadratic game.}
}

@InProceedings{hebbar25a,
    title = {Responding to Promises: No-regret learning against followers with memory},
    author = {Hebbar, Vijeth and Langbort, Cedric},
    pages = {647--659},
    abstract = {We consider a repeated Stackelberg game setup where the leader faces a sequence of followers of unknown types and must learn what commitments to make. While previous works have considered followers that best respond to the commitment announced by the leader in every round, we relax this setup in two ways. Motivated by natural scenarios where the leader's reputation factors into how the followers choose their response, we consider followers with memory. Specifically, we model followers that base their response on not just the leader's current commitment but on an aggregate of their past commitments. In developing learning strategies that the leader can employ against such followers, we make the second relaxation and assume boundedly rational followers -- in particular -- quantal responding followers. Interestingly, we observe that the smoothness property offered by the quantal response (QR) model helps in addressing the challenge posed by learning against followers with memory. Utilizing techniques from online learning, we develop algorithms that guarantee $O(\sqrt{T})$ regret for quantal responding memory-less followers and $O(\sqrt{BT})$ regret for followers with bounded memory of length $B$ with both scaling polynomially in game parameters.}
}

@InProceedings{bencherki25a,
    title = {Adaptive Control of Positive Systems with Application to Learning SSP},
    author = {Bencherki, Fethi and Rantzer, Anders},
    pages = {660--672},
    abstract = {An adaptive controller is proposed and analyzed for the class of infinite-horizon optimal control problems in positive linear systems presented in (Ohlin et al., 2024b). This controller is derived from the solution of a ``data-driven algebraic equation'' constructed using the model-free Bellman equation from Q-learning. The equation is driven by data correlation matrices that do not scale with the number of data points, enabling efficient online implementation. Consequently, a sufficient condition guaranteeing stability and robustness to unmodeled dynamics is established. The derived results also provide a quantitative characterization of the interplay between excitation level and robustness to unmodeled dynamics. The class of optimal control problems considered here is equivalent to Stochastic Shortest Path (SSP) problems, allowing for a performance comparison between the proposed adaptive policy and model-free algorithms for learning the stochastic shortest path, as demonstrated in the numerical experiment.}
}

@InProceedings{cummins25a,
    title = {DeePC-Hunt: Data-enabled Predictive Control Hyperparameter Tuning via Differentiable Optimization},
    author = {Cummins, Michael and Padoan, Alberto and Moffat, Keith and Dorfler, Florian and Lygeros, John},
    pages = {673--685},
    abstract = {This paper introduces Data-enabled Predictive Control Hyperparameter Tuning via Differentiable Optimization (DeePC-Hunt), a backpropagation-based method for automatic hyperparameter tuning of the DeePC algorithm. The necessity for such a method arises from the importance of hyperparameter selection to achieve satisfactory closed-loop DeePC performance. The standard methods for hyperparameter selection are to either optimize the open-loop performance, or use manual guess-and-check. Optimizing the open-loop performance can result in unacceptable closed-loop behavior, while manual guess-and-check can pose safety challenges. DeePC-Hunt provides an alternative method for hyperparameter tuning which uses an approximate model of the system dynamics and backpropagation to directly optimize hyperparameters for the closed-loop DeePC performance. Numerical simulations demonstrate the effectiveness of DeePC in combination with DeePC-Hunt in a complex stabilization task for a nonlinear system and its superiority over model-based control strategies in terms of robustness to model misspecifications.}
}

@InProceedings{dawood25a,
    title = {A Dynamic Safety Shield for Safe and Efficient Reinforcement Learning of Navigation Tasks},
    author = {Dawood, Murad and Shokry, Ahmed and Bennewitz, Maren},
    pages = {686--697},
    abstract = {Reinforcement learning (RL) has been successfully applied to a variety of robotics applications, where it outperforms classical methods. However, the safety aspect of RL and the transfer to the real world remain an open challenge. A prominent field for tackling this challenge and ensuring the safety of the agents during training and execution is safe reinforcement learning. Safe RL can be achieved through constrained RL and safe exploration approaches. The former learns the safety constraints over the course of training to achieve a safe behavior by the end of training, at the cost of high number of collisions at earlier stages of the training. The latter offers robust safety by enforcing the safety constraints as hard constraints, which prevents collisions but hinders the exploration of the RL agent, resulting in lower rewards and poor performance. To overcome those drawbacks, we propose a novel safety shield, that combines the robustness of the optimization-based controllers with the long prediction capabilities of the RL agents, allowing the RL agent to adaptively tune the parameters of the controller. Our approach is able to improve the exploration of the RL agents for navigation tasks, while minimizing the number of collisions. Experiments in simulation show that our approach outperforms state-of-the-art baselines in the reached goals-to-collisions ratio in different challenging environments. The goals-to-collisions ratio metrics emphasizes the importance of minimizing the number of collisions, while learning to accomplish the task. Our approach achieves a higher number of reached goals compared to the classic safety shields and fewer collisions compared to constrained RL approaches. Finally, we demonstrate the performance of the proposed method in a real-world experiment.}
}

@InProceedings{wang25e,
    title = {Multi-Constraint Safe Reinforcement Learning via Closed-form Solution for Log-Sum-Exp Approximation of Control Barrier Functions},
    author = {Wang, Chenggang and Wang, Xinyi and Dong, Yutong and Song, Lei and Guan, Xinping},
    pages = {698--710},
    abstract = {The safety of training task policies and their subsequent application using reinforcement learning (RL) methods has become a focal point in the field of safe RL. A central challenge in this area remains the establishment of theoretical guarantees for safety during both the learning and deployment processes. Given the successful implementation of Control Barrier Function (CBF)-based safety strategies in a range of control-affine robotic systems, CBF-based safe RL demonstrates significant promise for practical applications in real-world scenarios. However, integrating these two approaches presents several challenges. First, embedding safety optimization within the RL training pipeline requires that the optimization outputs be differentiable with respect to the input parameters, a condition often referred to as differentiable optimization, which is non-trivial to solve. Second, the differentiable optimization framework confronts significant efficiency issues, especially when dealing with multi-constraint problems. To address these challenges, this paper presents a CBF-based safe RL architecture that effectively mitigates the issues outlined above. The proposed approach constructs a continuous AND logic approximation for the multiple constraints using a single composite CBF. By leveraging this approximation, a close-form solution of the quadratic programming is derived for the policy network in RL, thereby circumventing the need for differentiable optimization within the end-to-end safe RL pipeline. This strategy significantly reduces computational complexity because of the closed-form solution while maintaining safety guarantees. Simulation results demonstrate that, in comparison to existing approaches relying on differentiable optimization, the proposed method significantly reduces training computational costs while ensuring provable safety throughout the training process. This advancement opens up promising potential for applications in large-scale optimization problems.}
}

@InProceedings{labbe25a,
    title = {Analytical Integral Global Optimization},
    author = {Labbe, Sebastien and Prete, Andrea Del},
    pages = {711--722},
    abstract = {Numerical optimization has been the workhorse powering the success of many machine learning and artificial intelligence tools over the last decade. However, current state-of-the-art algorithms for solving unconstrained non-convex optimization problems in high-dimensional spaces, either suffer from the curse of dimensionality as they rely on sampling, or get stuck in local minima as they rely on gradient-based optimization. We present a new graduated optimization method based on the optimization of the integral of the cost function over a region, which is incrementally shrunk towards a single point, recovering the original problem. We focus on the optimization of polynomial functions, for which the integral over simple regions (e.g. hyperboxes) can be computed efficiently. We show that this algorithm is guaranteed to converge to the global optimum in the simple case of a scalar decision variable. While this theoretical result does not extend to the multi-dimensional case, we empirically show that our approach outperforms several state-of-the-art algorithms when tested on sparse polynomial functions in dimensions up to 170 decision variables.}
}

@InProceedings{hu25b,
    title = {Efficient Duple Perturbation Robustness in Low-rank MDPs},
    author = {Hu, Yang and Ma, Haitong and Li, Na and Dai, Bo},
    pages = {723--737},
    abstract = {The pursuit of robustness has recently been a popular topic in reinforcement learning (RL) research, yet the existing methods generally suffer from computation issues that obstruct their real-world implementation. In this paper, we consider MDPs with low-rank structures, where the transition kernel can be written as a linear product of feature map and factors. We introduce *duple perturbation* robustness, i.e. perturbation on both the feature map and the factors, via a novel characterization of $(\xi,\eta)$-ambiguity sets featuring computational efficiency. Our novel low-rank robust MDP formulation is compatible with the low-rank function representation view, and therefore, is naturally applicable to practical RL problems with large or even continuous state-action spaces. Meanwhile, it also gives rise to a provably efficient and practical algorithm with theoretical convergence rate guarantee. Lastly, the robustness of our proposed approach is justified by numerical experiments, including classical control tasks with continuous state-action spaces.}
}

@InProceedings{lin25a,
    title = {Probabilistic Satisfaction of Temporal Logic Constraints in Reinforcement Learning via Adaptive Policy-Switching},
    author = {Lin, Xiaoshan and Yuksel, Sadik Bera and Yazicioglu, Yasin and Aksaray, Derya},
    pages = {738--749},
    abstract = {Constrained Reinforcement Learning (CRL) is a subset of machine learning that introduces constraints into the traditional reinforcement learning (RL) framework. Unlike conventional RL which aims solely to maximize cumulative rewards, CRL incorporates additional constraints that represent specific mission requirements or limitations that the agent must comply with during the learning process. In this paper, we address a type of CRL problem where an agent aims to learn the optimal policy to maximize reward while ensuring a desired level of temporal logic constraint satisfaction throughout the learning process. We propose a novel framework that relies on switching between pure learning (reward maximization) and constraint satisfaction. This framework estimates the probability of constraint satisfaction based on earlier trials and properly adjusts the probability of switching between learning and constraint satisfaction policies. We theoretically validate the correctness of the proposed algorithm and demonstrate its performance through comprehensive simulations.}
}

@InProceedings{lian25a,
    title = {Robust Inverse Reinforcement Learning Control with Unknown States},
    author = {Lian, Bosen and Xue, Wenqian and Nguyen, Nhan},
    pages = {750--762},
    abstract = {This paper designs a robust inverse reinforcement learning (IRL) algorithm that observes the expert's inputs and outputs to reconstruct the underlying cost function weights and optimal control policy for optimal discrete-time (DT) output feedback (OPFB) control systems while admitting disturbances and unknown states. The expert system is captured by a zero-sum game where its OPFB controller minimizes a cost function while robustly mitigating the effect of the worst disturbance, achieving a prescribed attenuation level. The inputs and outputs of the expert can be observed, but not the states. To enable the learner to replicate the behavior of the expert, we first develop a model-based IRL algorithm and subsequently design an equivalent model-free, data-driven version. This latter infers the quadratic cost function weights that can yield the expert's static OPFB control policy, using output and input data of both the expert and learner. The convergence of the proposed algorithms is rigorously validated through theoretical analysis and numerical experiments.}
}

@InProceedings{wang25f,
    title = {Mission-driven Exploration for Accelerated Deep Reinforcement Learning with Temporal Logic Task Specifications},
    author = {Wang, Jun and Hasanbeig, Hosein and Tan, Kaiyuan and Sun, Zihe and Kantaros, Yiannis},
    pages = {763--776},
    abstract = {This paper addresses the problem of designing control policies for agents with unknown stochastic dynamics and control objectives specified using Linear Temporal Logic (LTL). Recent Deep Reinforcement Learning (DRL) algorithms have aimed to compute policies that maximize the satisfaction probability of LTL formulas, but they often suffer from slow learning performance. To address this, we introduce a novel Deep Q-learning algorithm that significantly improves learning speed. The enhanced sample efficiency stems from a mission-driven exploration strategy that prioritizes exploration towards directions likely to contribute to mission success. Identifying these directions relies on an automaton representation of the LTL task as well as a learned neural network that partially models the agent-environment interaction. We provide comparative experiments demonstrating the efficiency of our algorithm on robot navigation tasks in unseen environments.}
}

@InProceedings{huang25a,
    title = {Toward Near-Globally Optimal Nonlinear Model Predictive Control via Diffusion Models},
    author = {Huang, Tzu-Yuan and Lederer, Armin and Hoischen, Nicolas and Brudigam, Jan and Xiao, Xuehua and Sosnowski, Stefan and Hirche, Sandra},
    pages = {777--790},
    abstract = {Achieving global optimality in nonlinear model predictive control (NMPC) is challenging due to the non-convex nature of the underlying optimization problem. Since commonly employed local optimization techniques depend on carefully chosen initial guesses, this non-convexity often leads to suboptimal performance resulting from local optima. To overcome this limitation, we propose a novel diffusion model-based approach for near-globally optimal NMPC consisting of an offline and an online phase. The offline phase employs a local optimizer to sample from the distribution of optimal NMPC control sequences along generated system trajectories through random initial guesses. Subsequently, the generated diverse dataset is used to train a diffusion model to reflect the multi-modal distribution of optima. In the online phase, the trained model is leveraged to efficiently perform a variant of random shooting optimization to obtain near-globally optimal control sequences without relying on any initial guesses or online NMPC solving. The effectiveness of our approach is illustrated in a numerical simulation indicating high performance benefits compared to direct neural network approximations of NMPC and significantly lower computation times than online solving NMPC using global optimizers.}
}

@InProceedings{roemer25a,
    title = {Diffusion Predictive Control with Constraints},
    author = {R{\"o}mer, Ralf and Rohr, Alexander von and Schoellig, Angela},
    pages = {791--803},
    abstract = {Diffusion models have become popular for policy learning in robotics due to their ability to capture high-dimensional and multimodal distributions. However, diffusion policies are stochastic and typically trained offline, limiting their ability to handle unseen and dynamic conditions where novel constraints not represented in the training data must be satisfied. To overcome this limitation, we propose diffusion predictive control with constraints (DPCC), an algorithm for diffusion-based control with explicit state and action constraints that can deviate from those in the training data. DPCC incorporates model-based projections into the denoising process of a trained trajectory diffusion model and uses constraint tightening to account for model mismatch. This allows us to generate constraint-satisfying, dynamically feasible, and goal-reaching trajectories for predictive control. We show through simulations of a robot manipulator that DPCC outperforms existing methods in satisfying novel test-time constraints while maintaining performance on the learned control task.}
}

@InProceedings{henzinger25a,
    title = {Predictive Monitoring of Black-Box Dynamical Systems},
    author = {Henzinger, Thomas A. and Kresse, Fabian and Mallik, Kaushik and Yu, Emily and \v{Z}ikeli\'{c}, \DJ{}or\dj{}e},
    pages = {804--816},
    abstract = {We study the problem of predictive runtime monitoring of black-box dynamical systems with quantitative safety properties. The black-box setting stipulates that the exact semantics of the dynamical system and the controller are unknown, and that we are only able to observe the state of the controlled (aka, closed-loop) system at finitely many time points. We present a novel framework for predicting future states of the system based on the states observed in the past. The numbers of past states and of predicted future states are parameters provided by the user. Our method is based on a combination of Taylor's expansion and the backward difference operator for numerical differentiation. We also derive an upper bound on the prediction error under the assumption that the system dynamics and the controller are smooth. The predicted states are then used to predict safety violations ahead in time. Our experiments demonstrate practical applicability of our method for complex black-box systems, showing that it is computationally lightweight and yet significantly more accurate than the state-of-the-art predictive safety monitoring techniques.}
}

@InProceedings{wang25c,
    title = {Federated Posterior Sharing for Multi-Agent Systems in Uncertain Environments},
    author = {Wang, Yuxi and Wu, Peng and Imani, Mahdi},
    pages = {817--829},
    abstract = {The use of artificial intelligence (AI) agents is increasingly growing in complex, dynamic environments such as disaster response, search and rescue, and law enforcement. These domains are often only partially known, requiring agents to learn and adapt as they gather more information. In multi-agent settings, where agents operate independently and possess diverse, partial views of the environment, sharing their environmental knowledge is essential for enhancing operational efficiency and safety. Existing federated learning approaches focus primarily on policy sharing without modeling environmental uncertainty. To address this gap, this paper presents a framework that enables multiple agents to collaboratively share their probabilistic knowledge of the environment, building a global, shared understanding that efficiently guides their policies. Unlike existing data fusion techniques that exchange raw data--posing privacy risks and increasing communication costs--our method fuses agents' local posterior distributions as an abstract representation of their past data. We provide both single-step and multi-step synchronization, enabling recursive aggregation of agents' knowledge to support informed and adaptive decision-making. Numerical experiments show that our method achieves superior accuracy and decision efficiency compared to several existing methods, particularly in settings with heterogeneous priors and greater uncertainty.}
}

@InProceedings{ravari25a,
    title = {Hybrid Modeling of Heterogeneous Human Teams for Collaborative Decision Processes},
    author = {Ravari, Amirhossein and Ghoreishi, Seyede Fatemeh and Lan, Tian and Bastian, Nathaniel D. and Imani, Mahdi},
    pages = {830--843},
    abstract = {The increasing integration of artificial intelligence (AI) enabled systems with human operators underscores the need for seamless collaboration across various domains. Accurate modeling of human behavior can enable AI-enabled systems to anticipate human decisions and align themselves to support humans in complex tasks. Unlike existing methods focusing primarily on individual human behavioral modeling, this paper models human behavior within heterogeneous teams working toward a cooperative objective. In such teams, members often have varying skills, knowledge, and levels of awareness, which influence their decision-making processes. This paper models team behavior as a sub-optimal, hybrid form of multi-agent reinforcement learning. By leveraging centralized training with hybrid centralized/decentralized execution, the model captures a spectrum of team behaviors, from fully centralized to fully decentralized and in between. This paper quantitatively models each team member's awareness and communication levels, enabling the inverse learning of these parameters from observed human team behavior data. Numerical experiments validate the robustness and accuracy of the framework across diverse scenarios and team compositions, underscoring its effectiveness in modeling complex human interactions.}
}

@InProceedings{zhang25a,
    title = {Deep Source-Seekers with Obstacle Avoidance: Adaptive Hybrid Control with Transformers In-The-Loop},
    author = {Zhang, Xiyuan and Ochoa, Daniel and Talonia, Regina and Poveda, Jorge},
    pages = {844--855},
    abstract = {Autonomous signal source localization is a cornerstone of modern robotics, underpinning critical applications in environmental monitoring, search and rescue, and industrial automation. Traditional source-seeking methods, such as gradient-based algorithms and potential field-based approaches, often struggle with local minimum entrapment in environments cluttered with obstacles. To address these challenges, in this paper we introduce a novel model-free approach that combines a perception-driven hybrid controller---integrating adaptive continuous-time and discrete-time feedback---with an Environmental Complexity Adapter (ECA) for perception model selection. The proposed dynamics implement real-time exploration/exploitation mechanisms and complementary deep learning-based perception architectures: YOLOv10 for rapid and accurate object detection in clear conditions, and Real-Time DEtection TRansformer (RT-DETR) for enhanced robustness in noisy environments. By continuously assessing the quality of sensor data, the ECA dynamically switches between these models, optimizing the trade-off between processing speed and detection reliability. This approach harnesses the robustness of hybrid controllers while enabling efficient, perception-guided source-seeking and obstacle avoidance in complex environments. Extensive numerical simulations validate the effectiveness of the proposed approach.},
}

@InProceedings{anderson25a,
    title = {Learning with contextual information in non-stationary environments},
    author = {Anderson, Sean and Hespanha, Joao P.},
    pages = {856--868},
    abstract = {We consider a repeated decision-making setting in which the decision maker has access to contextual information and lacks a model or a priori knowledge of the relationship between the actions, context, and costs that they aim to minimize. Moreover, we assume that the environment may be non-stationary due to the presence of other agents that may be reacting to our decisions. We propose an algorithm inspired by log-linear learning that uses Boltzmann distributions to generate stochastic policies. We consider two general notions of context and provide regret bounds for each: 1) a finite number of possible measurements and 2) a continuum of measurements that weight a set of finite classes. In the non-stationary setting, we incur some regret but can make it arbitrarily small. We illustrate the operation of the algorithm through two examples: one that uses synthetic data (based on the rock-paper-scissors game) and another that uses real data for malware classification. Both examples exhibit (by construction or naturally) significant lack of stationarity.}
}

@InProceedings{jung25a,
    title = {Contingency Constrained Planning with MPPI within MPPI},
    author = {Jung, Leonard and Estornell, Alexander and Everett, Michael},
    pages = {869--880},
    abstract = {For safety, autonomous systems must be able to consider sudden changes and enact contingency plans appropriately. State-of-the-art methods currently find trajectories that balance between nominal and contingency behavior, or plan for a singular contingency plan; however, this does not guarantee that the resulting plan is safe for all time. To address this research gap, this paper presents Contingency-MPPI, a data-driven optimization-based strategy that embeds contingency planning inside a nominal planner. By learning to approximate the optimal contingency-constrained control sequence with adaptive importance sampling, the proposed method's sampling efficiency is further improved with initializations from a lightweight path planner and trajectory optimizer. Finally, we present simulated and hardware experiments demonstrating our algorithm generating nominal and contingency plans in real time on a mobile robot.}
}

@InProceedings{xiao25a,
    title = {Computing Quasi-Nash Equilibria via Gradient-Response Schemes},
    author = {Xiao, Zhuoyu and Shanbhag, Uday V.},
    pages = {881--893},
    abstract = {We consider a class of smooth static N-player noncooperative games, where player objectives are expectation-valued and potentially nonconvex. In such a setting, we consider the largely open question of efficiently computing a suitably defined quasi-Nash equilibrium (QNE) via a stochastic gradient-response framework. First, under a suitably defined quadratic growth property, we prove that both the stochastic synchronous gradient-response (SSGR) scheme and its asynchronous counterpart (SAGR) are characterized by almost sure convergence to a QNE and a sublinear rate guarantee. Notably, when a potentiality requirement is overlaid under a somewhat stronger pseudomonotonicity condition, this claim can be made for a Nash equilibrium (NE), rather than a QNE. Second, under the weak sharpness property, we show that the deterministic synchronous variant (SGR) displays a linear rate of convergence sufficiently close to a QNE by leveraging a geometric decay in steplengths. This suggests the development of a two-stage scheme with global non-asymptotic sublinear rates and a local linear rate. We also present applications satisfying the prescribed requirements where preliminary numerics appear promising.}
}

@InProceedings{koprulu25a,
    title = {Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations},
    author = {Koprulu, Cevahir and Li, Po-Han and Qiu, Tianyu and Zhao, Ruihan and Westenbroek, Tyler and Fridovich-Keil, David and Chinchali, Sandeep and Topcu, Ufuk},
    pages = {894--906},
    abstract = {Many continuous control problems can be formulated as sparse-reward reinforcement learning (RL) tasks. In principle, online RL methods can automatically explore the state space to solve each new task. However, discovering sequences of actions that lead to a non-zero reward becomes exponentially more difficult as the task horizon increases. Manually shaping rewards can accelerate learning for a fixed task, but it is an arduous process that must be repeated for each new environment. We introduce a systematic reward-shaping framework that distills the information contained in 1) a task-agnostic prior data set and 2) a small number of task-specific expert demonstrations, and then uses these priors to synthesize dense dynamics-aware rewards for the given task. This supervision substantially accelerates learning in our experiments, and we provide analysis demonstrating how the approach can effectively guide online learning agents to faraway goals.}
}

@InProceedings{fujinami25a,
    title = {Domain Randomization is Sample Efficient for Linear Quadratic Control},
    author = {Fujinami, Tesshu and Lee, Bruce D. and Matni, Nikolai and Pappas, George J.},
    pages = {907--919},
    abstract = {We study the sample efficiency of domain randomization and robust control for the benchmark problem of learning the linear quadratic regulator (LQR). Domain randomization, which synthesizes controllers by minimizing average performance over a distribution of model parameters, has achieved empirical success in robotics, but its theoretical properties remain poorly understood. We establish that with an appropriately chosen sampling distribution, domain randomization achieves the optimal asymptotic rate of decay in the excess cost, matching certainty equivalence. We further demonstrate that robust control, while potentially overly conservative, exhibits superior performance in the low-data regime due to its ability to stabilize uncertain systems with coarse parameter estimates. We propose a gradient-based algorithm for domain randomization that performs well in numerical experiments, which enables us to validate the trends predicted by our analysis. These results provide insights into the use of domain randomization in learning-enabled control, and highlight several open questions about its application to broader classes of systems.}
}

@InProceedings{baheri25a,
    title = {WAVE: Wasserstein Adaptive Value Estimation for Actor-Critic Reinforcement Learning},
    author = {Baheri, Ali and Sharooei, Zahra and Salgarkar, Chirayu},
    pages = {920--931},
    abstract = {We present WAVE (Wasserstein Adaptive Value Estimation for Actor-Critic), an approach to enhance stability in deep reinforcement learning through adaptive Wasserstein regularization. Our method addresses the inherent instability of actor-critic algorithms by incorporating an adaptively weighted Wasserstein regularization term into the critic's loss function. We prove that WAVE achieves $\mathcal{O}\left(\frac{1}{k}\right)$ convergence rate for the critic's mean squared error and provide theoretical guarantees for stability through Wasserstein-based regularization. Using the Sinkhorn approximation for computational efficiency, our approach automatically adjusts the regularization based on the agent's performance. Theoretical analysis and experimental results demonstrate that WAVE achieves superior performance compared to standard actor-critic methods.}
}

@InProceedings{kim25b,
    title = {Realizable Continuous-Space Shields for Safe Reinforcement Learning},
    author = {Kim, Kyungmin and Corsi, Davide and Rodr\'{\i}guez, Andoni and Lanier, Jb and Parellada, Benjami and Baldi, Pierre and S\'{a}nchez, C\'{e}sar and Fox, Roy},
    pages = {932--945},
    abstract = {While Deep Reinforcement Learning (DRL) has achieved remarkable success across various domains, it remains vulnerable to occasional catastrophic failures without additional safeguards. An effective solution to prevent these failures is to use a shield that validates and adjusts the agent's actions to ensure compliance with a provided set of safety specifications. For real-world robotic domains, it is essential to define safety specifications over continuous state and action spaces to accurately account for system dynamics and compute new actions that minimally deviate from the agent's original decision. In this paper, we present the first shielding approach specifically designed to ensure the satisfaction of safety requirements in continuous state and action spaces, making it suitable for practical robotic applications. Our method builds upon realizability, an essential property that confirms the shield will always be able to generate a safe action for any state in the environment. We formally prove that realizability can be verified for stateful shields, enabling the incorporation of non-Markovian safety requirements, such as loop avoidance. Finally, we demonstrate the effectiveness of our approach in ensuring safety without compromising the policy's success rate by applying it to a navigation problem and a multi-agent particle environment.}
}

@InProceedings{gouru25a,
    title = {LiveNet: Robust, Minimally Invasive Multi-Robot Control for Safe and Live Navigation in Constrained Environments},
    author = {Gouru, Srikar and Lakkoju, Siddharth and Chandra, Rohan},
    pages = {946--958},
    abstract = {Robots in densely populated real-world environments frequently encounter constrained and cluttered situations such as passing through narrow doorways, hallways, and corridor intersections, where conflicts over limited space result in collisions or deadlocks among the robots. Current decentralized state-of-the-art optimization- and neural network-based approaches (i) are predominantly designed for general open spaces, and (ii) are overly conservative, either guaranteeing safety, or liveness, but not both. While some solutions rely on centralized conflict resolution, their highly invasive trajectories make them impractical for real-world deployment. This paper introduces LiveNet, a fully decentralized and robust neural network controller that enables human-like yielding and passing, resulting in agile, non-conservative, deadlock-free, and safe, navigation in congested, conflict-prone spaces. LiveNet is minimally invasive, without requiring inter-agent communication or cooperative behavior. The key insight behind LiveNet is a unified CBF formulation for simultaneous safety and liveness, which we integrate within a neural network for robustness. We evaluated LiveNet in simulation and found that general multi-robot optimization- and learning-based navigation methods fail to even reach the goal, and while methods designed specially for such environments do succeed, they are 10--20$\times$ slower, 4--5$\times$ more invasive, and much less robust to variations in the scenario configuration such as changes in the start states and goal states, among others. We open-source the LiveNet code at https://github.com/srikarg89/LiveNet.}
}

@InProceedings{kalaria25a,
    title = {$\alpha$-RACER: Real-Time Algorithm for Game-Theoretic Motion Planning and Control in Autonomous Racing using Near-Potential Function},
    author = {Kalaria, Dvij and Maheshwari, Chinmay and Sastry, Shankar},
    pages = {959--972},
    abstract = {Autonomous racing extends beyond the challenge of controlling a racecar at its physical limits. Professional racers employ strategic maneuvers to outwit other competing opponents to secure victory. While modern control algorithms can achieve human-level performance by computing offline racing lines for single-car scenarios, research on real-time algorithms for multi-car autonomous racing is limited. To bridge this gap, we develop game-theoretic modeling framework that incorporates the competitive aspect of autonomous racing like overtaking and blocking through a novel policy parametrization, while operating the car at its limit. Furthermore, we propose an algorithmic approach to compute the (approximate) Nash equilibrium strategy, which represents the optimal approach in the presence of competing agents. Specifically, we introduce an algorithm inspired by recently introduced framework of dynamic near-potential function, enabling real-time computation of the Nash equilibrium. Our approach comprises two phases: offline and online. During the offline phase, we use simulated racing data to learn a near-potential function that approximates utility changes for agents. This function facilitates the online computation of approximate Nash equilibria by maximizing its value. We evaluate our method in a head-to-head 3-car racing scenario, demonstrating superior performance compared to several existing baselines.}
}

@InProceedings{ajeyemi25a,
    title = {Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers},
    author = {Ajeyemi, Damola and Jafarpour, Saber and Dall'Anese, Emiliano},
    pages = {973--986},
    abstract = {Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and the emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of the closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings.}
}

@InProceedings{paluch25a,
    title = {A-NC: Adaptive Neural Control with implicit online inference of privileged parameters},
    author = {Paluch, Marcin and Bolli, Florian and Moure, Pehuen and Deng, Xiang and Delbruck, Tobi},
    pages = {987--998},
    abstract = {Rapid changes in the environment and robot parameters pose significant challenges for control systems, particularly when key parameters are not directly measurable. In this paper, we introduce a novel approach using classical Recurrent Neural Network (RNN) controllers to dynamically adapt policies in response to these changes. We propose strategies for data collection and processing that enable the successful training of efficient Gated Recurrent Unit (GRU) nonlinear controllers capable of adapting to changing parameters. We demonstrate this approach using a simulated and a physical cartpole robot. The RNNs are trained through supervised learning on data generated in simulation using Nonlinear Model Predictive Control (NMPC). We vary the cartpole's angle sensor offset or pole length jointly with pole mass, none of which are directly measurable by the robot. Our results show how the RNN controller adjusts its policy based on past trajectories, leading to control that mimics the NMPC, outperforming Domain Randomization (DR) technique applied to feedforward neural networks. Unlike NMPC, which relies on explicit knowledge of environment parameters, the RNN implicitly estimates these parameters from past trajectories, allowing it to adapt its control policy dynamically. It also outperforms NMPC control performance when the parameters relevant for NMPC are not known.}
}

@InProceedings{bousias25a,
    title = {Symmetries-enhanced Multi-Agent Reinforcement Learning},
    author = {Bousias, Nikolaos and Pertigkiozoglou, Stefanos and Daniilidis, Kostas and Pappas, George},
    pages = {999--1011},
    abstract = {Multi-agent reinforcement learning has emerged as a powerful framework for enabling agents to learn complex, coordinated behaviors but faces persistent challenges regarding its generalization, scalability and sample efficiency. Recent advancements have sought to alleviate those issues by embedding intrinsic symmetries of the systems in the policy. Yet, most dynamical systems exhibit little to no symmetries to exploit. This paper presents a novel framework for embedding extrinsic symmetries in multi-agent system dynamics that enables the use of symmetry-enhanced methods to address systems with insufficient intrinsic symmetries, expanding the scope of equivariant learning to a wide variety of MARL problems. Central to our framework is the Group Equivariant Graphormer, a group-modular architecture specifically designed for distributed swarming tasks. Extensive experiments on a swarm of symmetry-breaking quadrotors validate the effectiveness of our approach, showcasing its potential for improved generalization and zero-shot scalability. Our method achieves significant reductions in collision rates and enhances task success rates across a diverse range of scenarios and varying swarm sizes.}
}

@InProceedings{al-tawaha25a,
    title = {A Dynamic Penalization Framework for Online Rank-1 Semidefinite Programming Relaxations},
    author = {Al-Tawaha, Ahmad and Lavaei, Javad and Jin, Ming},
    pages = {1012--1024},
    abstract = {We propose a dynamic penalization framework for recovering rank-1 solutions in sequential semidefinite programming (SDP) relaxations. Obtaining rank-1 solutions--crucial for recovering physically meaningful solutions in many applications--becomes particularly challenging in dynamic environments where problem parameters continuously evolve. Our framework operates in two interconnected phases: the learning phase dynamically adjusts penalty parameters to enforce rank-1 feasibility based on feedback from the decision phase, while the decision phase solves the resulting penalized SDP relaxations using the penalty parameters specified by the learning phase. To accelerate rank-1 recovery across sequential problems, we introduce a meta-learning model that provides informed initializations for the penalty matrices. The meta-learning model leverages historical data from previously solved tasks, eliminating the need for externally curated datasets. By using task-specific features and updates from prior iterations, the meta-model intelligently initializes penalty parameters, reducing the number of iterations required between the two phases. We prove sublinear convergence to rank-1 solutions and establish low dynamic regret bounds that improve with task similarity. Empirical results on real-world rank-constrained applications, including the Max-Cut problem and Optimal Power Flow (OPF), demonstrate that our method consistently recovers rank-1 solutions.}
}

@InProceedings{tang25a,
    title = {Meta-Learning for Adaptive Control with Automated Mirror Descent},
    author = {Tang, Sunbochen and Sun, Haoyuan and Azizan, Navid},
    pages = {1025--1037},
    abstract = {Adaptive control achieves concurrent parameter learning and stable control under uncertainties that are linearly parameterized with known nonlinear features. Nonetheless, it is often difficult to obtain such nonlinear features. To address this difficulty, recent progress has been made in integrating meta-learning with adaptive control to learn such nonlinear features from data. However, these meta-learning-based control methods rely on classical adaptation laws using gradient descent, which is confined to the Euclidean geometry. In this paper, we propose a novel method that combines meta-learning and adaptation laws based on mirror descent, a popular generalization of gradient descent, which takes advantage of the potentially non-Euclidean geometry of the parameter space. In our approach, meta-learning not only learns the nonlinear features but also searches for a suitable mirror-descent potential function that optimizes control performance. Through numerical simulations, we demonstrate the effectiveness of the proposed method in learning efficient representations and real-time tracking control performance under uncertain dynamics.}
}

@InProceedings{bevanda25a,
    title = {Kernel-Based Optimal Control: An Infinitesimal Generator Approach},
    author = {Bevanda, Petar and Hoischen, Nicolas and Wittmann, Tobias and Brudigam, Jan and Hirche, Sandra and Houska, Boris},
    pages = {1038--1052},
    abstract = {This paper presents a novel operator-theoretic approach for optimal control of nonlinear stochastic systems within reproducing kernel Hilbert spaces. Our learning framework leverages data samples of system dynamics and stage cost functions, with only control penalties and constraints provided. The proposed method directly learns the infinitesimal generator of a controlled stochastic diffusion in an infinite-dimensional hypothesis space. We demonstrate that our approach seamlessly integrates with modern convex operator-theoretic Hamilton-Jacobi-Bellman recursions, enabling a data-driven solution to the optimal control problems. Furthermore, our learning framework includes nonparametric estimators for uncontrolled infinitesimal generators as a special case. Numerical experiments, ranging from synthetic differential equations to simulated robotic systems, showcase the advantages of our approach compared to both modern data-driven and classical nonlinear programming methods for optimal control.}
}

@InProceedings{li25b,
    title = {Lyapunov Perception Contracts for Operating Design Domains},
    author = {Li, Yangge and Ji, Chenxi and Anchalia, Jai and Jia, Yixuan and Yang, Benjamin C and Zhuang, Daniel and Mitra, Sayan},
    pages = {1053--1065},
    abstract = {There are two barriers to assessing the reliability of visual control systems that use machine learning (ML) models for perception and state estimation. First, the reasoning has to include the image rendering process, which is affected by environmental factors such as lighting and weather in complex ways. Second, we lack meaningful specifications for ML models like deep neural networks (DNNs). In this paper, we introduce Lyapunov Perception Contracts (LPC) as a method to address these challenges. We show how these contracts can be used as specifications for DNN-based state estimators, which assure closed-loop stability. We propose a method for synthesizing LPC from data and the models for the controller and plant dynamics. We also show how LPCs can be used to find operating design domains for visual controllers that operate in finitely parameterized environments. We illustrate applications of this method in a visual automated landing system using both data from simulations and GoogleEarth.}
}

@InProceedings{wang25b,
    title = {Neuro-Symbolic Deadlock Resolution in Multi-Robot Systems},
    author = {Wang, Ruiyang and He, Bowen and Pajic, Miroslav},
    pages = {1066--1077},
    abstract = {This work addresses the problem of deadlock situations that are common in decentralized multi-robot missions. Existing approaches focus on predicting potential deadlocks and intervening before they actually occur. However, these methods often struggle to detect all possible deadlocks, especially in environments with uncontrollable obstacles, and may inevitably introduce new dead-locks after interventions. Consequently, we propose a neuro-symbolic deadlock resolution (NSDR) method based on Neural Logic Machines (NLMs). NSDR is designed specifically to resolve dead-locks after their occurrence, with the guarantee that no further persistent deadlocks will emerge after the initial resolution in environments with or without obstacles. Our approach leverages the similarity in logic rules when resolving simple deadlocks involving a small number of robots; this facilitates their use when resolving more complex scenarios with larger robot groups. Training NSDR on simpler deadlock cases allows it to generalize and effectively resolve more complex situations by utilizing the logic rules it has learned from simple deadlocks. We thoroughly evaluate the method in case studies with varying numbers of robots involved in deadlocks and show that NSDR outperforms the state of the art methods, which are based on the use of the adaptive repulsive force.}
}

@InProceedings{mandal25a,
    title = {A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks},
    author = {Mandal, Saptarshi and Lin, Xiaojun and Srikant, Rayadurgam},
    pages = {1078--1089},
    abstract = {Knowledge distillation, where a small student model learns from a pre-trained large teacher model, has achieved substantial empirical success since the seminal work of (Hinton et al., 2015). Despite prior theoretical studies exploring the benefits of knowledge distillation, an important question remains unanswered: why does soft-label training from the teacher require significantly fewer neurons than directly training a small neural network with hard labels? To address this, we first present motivating experimental results using simple neural network models on a binary classification problem. These results demonstrate that soft-label training consistently outperforms hard-label training in accuracy, with the performance gap becoming more pronounced as the dataset becomes increasingly difficult to classify. We then substantiate these observations with a theoretical contribution based on two-layer neural network models. Specifically, we show that soft-label training using gradient descent requires only \(O\left(\frac{1}{\gamma^2 \epsilon}\right)\) neurons to achieve a classification loss averaged over epochs smaller than some \(\epsilon > 0\), where \(\gamma\) is the separation margin of the limiting kernel. In contrast, hard-label training requires \(O\left(\frac{1}{\gamma^4} \cdot \ln\left(\frac{1}{\epsilon}\right)\right)\) neurons, as derived from an adapted version of the gradient descent analysis in (Ji and Telgarsky, 2020). This implies that when \(\gamma \leq \epsilon\), i.e., when the dataset is challenging to classify, the neuron requirement for soft-label training can be significantly lower than that for hard-label training. Finally, we present experimental results on deep neural networks, further validating these theoretical findings.}
}

@InProceedings{williams25a,
    title = {QP Based Constrained Optimization for Reliable PINN Training},
    author = {Williams, Alan and Leon, Christopher and Scheinker, Alexander},
    pages = {1090--1101},
    abstract = {Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for integrating physics-based constraints and data to address forward and inverse problems in machine learning. Despite their potential, the implementation of PINNs are hampered by several challenges, including issues related to convergence, stability, and the design of neural networks and loss functions. In this paper, we introduce a novel training scheme that addresses these challenges by framing the training process as a constrained optimization problem. Utilizing a quadratic program (QP)-based gradient descent law, our approach simplifies the design of loss functions and guarantees convergences to optimal neural network parameters. This methodology enables dynamic balancing, over the course of training, between data-based loss and a partial differential equation (PDE) residual loss, ensuring an acceptable level of accuracy while prioritizing the minimization of PDE-based loss. We demonstrate the formulation of the constrained PINNs approach with noisy data, in the context of solving Laplace's equation in a capacitor with complex geometry. This work not only advances the capabilities of PINNs but also provides a framework for their training.}
}

@InProceedings{yang25b,
    title = {STLGame: Signal Temporal Logic Games in Adversarial Multi-Agent Systems},
    author = {Yang, Shuo and Zheng, Hongrui and Vasile, Cristian-Ioan and Pappas, George and Mangharam, Rahul},
    pages = {1102--1114},
    abstract = {We study how to synthesize a robust and safe policy for autonomous systems under signal temporal logic (STL) tasks in adversarial settings against unknown dynamic agents. To ensure the worstcase STL satisfaction, we propose STLGame, a framework that models the multi-agent system as a two-player zero-sum game, where the ego agents try to maximize the STL satisfaction and other agents minimize it. STLGame aims to find a Nash equilibrium policy profile, which is the best case in terms of robustness against unseen opponent policies, by using the fictitious self-play (FSP) framework. FSP iteratively converges to a Nash profile, even in games set in continuous stateaction spaces. We propose a gradient-based method with differentiable STL formulas, which is crucial in continuous settings to approximate the best responses at each iteration of FSP. We show this key aspect experimentally by comparing with reinforcement learning-based methods to find the best response. Experiments on two standard dynamical system benchmarks, Ackermann steering vehicles and autonomous drones, demonstrate that our converged policy is almost unexploitable and robust to various unseen opponents' policies. All code and additional experimental results can be found on our project website: https://sites.google.com/view/stlgame}
}

@InProceedings{kim25d,
    title = {CIKAN: Constraint Informed Kolmogorov-Arnold Networks for Autonomous Spacecraft Rendezvous using Time Shift Governor},
    author = {Kim, Taehyeun and Girard, Anouck and Kolmanovsky, Ilya},
    pages = {1115--1126},
    abstract = {This paper considers a Constraints-Informed Neural Network (CINN) approximation for the Time Shift Governor (TSG), which is an add-on scheme to the nominal closed-loop system used to enforce constraints by time-shifting the reference trajectory in spacecraft rendezvous applications. We incorporate Kolmogorov-Arnold Networks (KANs), an emerging architecture in the AI community, as a fundamental component of CINN and propose a Constraints-Informed Kolmogorov-Arnold Network (CIKAN)-based approximation for TSG. We demonstrate the effectiveness of the CIKAN-based TSG through simulations of constrained spacecraft rendezvous missions on highly elliptic orbits and present comparisons between CIKANs, MLP-based CINNs, and the conventional TSG.}
}

@InProceedings{zeng25a,
    title = {Data-driven optimal control of unknown nonlinear dynamical systems using the Koopman operator},
    author = {Zeng, Zhexuan and Zhou, Ruikun and Meng, Yiming and Liu, Jun},
    pages = {1127--1139},
    abstract = {Nonlinear optimal control is vital for numerous applications but remains challenging for unknown systems due to the difficulties in accurately modelling dynamics and handling computational demands, particularly in high-dimensional settings. This work develops a theoretically certifiable framework that integrates a modified Koopman operator approach with model-based reinforcement learning to address these challenges. By relaxing the requirements on observable functions, our method incorporates nonlinear terms involving both states and control inputs, significantly enhancing system identification accuracy. Moreover, by leveraging the power of neural networks to solve partial differential equations (PDEs), our approach is able to achieving stabilizing control for high-dimensional dynamical systems, up to 9-dimensional. The learned value function and control laws are proven to converge to those of the true system at each iteration. Additionally, the accumulated cost of the learned control closely approximates that of the true system, with errors ranging from $10^{-5}$ to $10^{-3}$.}
}

@InProceedings{he25a,
    title = {Imperative MPC: An End-to-End Self-Supervised Learning with Differentiable MPC for UAV Attitude Control},
    author = {He, Haonan and Qiu, Yuheng and Geng, Junyi},
    pages = {1140--1153},
    abstract = {Modeling and control of nonlinear dynamics are critical in robotics, especially in scenarios with unpredictable external influences and complex dynamics. Traditional cascaded modular control pipelines often yield suboptimal performance due to conservative assumptions and tedious parameter tuning. Pure data-driven approaches promise robust performance but suffer from low sample efficiency, sim-to-real gaps, and reliance on extensive datasets. Hybrid methods combining learning-based and traditional model-based control in an end-to-end manner offer a promising alternative. This work presents a self-supervised learning framework combining learning-based inertial odometry (IO) module and differentiable model predictive control (d-MPC) for Unmanned Aerial Vehicle (UAV) attitude control. The IO denoises raw IMU measurements and predicts UAV attitudes, which are then optimized by MPC for control actions in a bi-level optimization (BLO) setup, where the inner MPC optimizes control actions and the upper level minimizes discrepancy between real-world and predicted performance. The framework is thus end-to-end and can be trained in a self-supervised manner. This approach combines the strength of learning-based perception with the interpretable model-based control. Results show the effectiveness even under strong wind. It can simultaneously enhance both the MPC parameter learning and IMU prediction performance.}
}

@InProceedings{begzadic25a,
    title = {Back to Base: Towards Hands-Off Learning via Safe Resets with Reach-Avoid Safety Filters},
    author = {Begzadic, Azra and Shinde, Nikhil and Tonkens, Sander and Hirsch, Dylan and Ugalde, Kaleb and Yip, Michael and Cortes, Jorge and Herbert, Sylvia},
    pages = {1154--1166},
    abstract = {Designing controllers to accomplish a task while guaranteeing constraints on safety remains a significant challenge. We often want an agent to perform well in a nominal task, such as environment exploration, while ensuring it can avoid unsafe states and return to a desired target by a specific time. In particular we are motivated by the setting of safe, efficient, hands-off training for reinforcement learning in the real world. By enabling a robot to safely and autonomously reset to a desired region (e.g., charging stations) without human intervention, we can enhance efficiency and facilitate training. Safety filters, such as those based on control barrier functions, enable decoupling safety from nominal control objectives and rigorously guaranteeing safety. Despite their success, constructing these functions for general nonlinear systems with control constraints and system uncertainties remains an open problem. This paper introduces a safety filter obtained from the value function associated with the reach-avoid problem. The proposed safety filter minimally modifies the nominal controller while avoiding unsafe regions and guiding the system back to the desired target set. By preserving policy performance while allowing safe resetting, we enable efficient hands-off reinforcement learning and advance the feasibility of safe training for real world robots. We demonstrate our approach using a modified version of soft actor-critic to safely train a swing-up task on a modified cartpole stabilization problem.}
}

@InProceedings{samari25a,
    title = {Abstraction-Based Control of Unknown Continuous-Space Models with Just Two Trajectories},
    author = {Samari, Behrad and Zaker, Mahdieh and Lavaei, Abolfazl},
    pages = {1167--1179},
    abstract = {Finite abstractions (a.k.a. symbolic models) offer an effective scheme for approximating the complex continuous-space systems with simpler models in the discrete-space domain. A crucial aspect, however, is to establish a formal relation between the original system and its symbolic model, ensuring that a discrete controller designed for the symbolic model can be effectively implemented as a hybrid controller (using an interface map) for the original system. This task becomes even more challenging when the exact mathematical model of the continuous-space system is unknown. To address this, the existing literature mainly employs scenario-based data-driven methods, which require collecting a large amount of data from the original system. In this work, we propose a data-driven framework that utilizes only two input-state trajectories collected from unknown nonlinear polynomial systems to synthesize a hybrid controller, enabling the desired behavior on the unknown system through the controller derived from its symbolic model. To accomplish this, we employ the concept of alternating simulation functions (ASFs) to quantify the closeness between the state trajectories of the unknown system and its data-driven symbolic model. By satisfying a specific rank condition on the collected data, which intuitively ensures that the unknown system is persistently excited, we directly design an ASF and its corresponding hybrid controller using finite-length data without explicitly identifying the unknown system, while providing correctness guarantees. This is achieved through proposing a data-based sum-of-squares (SOS) optimization program, enabling a systematic approach to the design process. We illustrate the effectiveness of our data-driven approach through a case study.}
}

@InProceedings{elahi25a,
    title = {Reinforcement Learning from Multi-level and Episodic Human Feedback},
    author = {Elahi, Muhammad Qasim and Oguchienti, Somtochukwu and Ahmed, Maheed H. and Ghasemi, Mahsa},
    pages = {1180--1193},
    abstract = {Designing an effective reward function has long been a challenge in reinforcement learning, particularly for complex tasks in unstructured environments. To address this, various learning paradigms have emerged that leverage different forms of human input to specify or refine the reward function. Reinforcement learning from human feedback is a prominent approach that utilizes human comparative feedback---expressed as a preference for one behavior over another---to tackle this problem. In contrast to comparative feedback, we explore multi-level human feedback, which is provided in the form of a score at the end of each episode. This type of feedback offers more coarse but informative signals about the underlying reward function than binary feedback. Additionally, it can handle non-Markovian rewards, as it is based on an entire episode's evaluation. We propose an algorithm to efficiently learn both the reward function and the optimal policy from this form of feedback. Moreover, we show that the proposed algorithm achieves sublinear regret and demonstrate its empirical effectiveness through extensive simulations.}
}

@InProceedings{brindise25a,
    title = {``What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives},
    author = {Brindise, Noel and Hebbar, Vijeth and Shah, Riya and Langbort, Cedric},
    pages = {1194--1205},
    abstract = {In this work, we present a new approach to explainable Reinforcement Learning called Diverse Near-Optimal Alternatives (DNA). DNA seeks a set of reasonable "options" for trajectory-planning agents, optimizing policies to produce qualitatively diverse trajectories in Euclidean space. In the spirit of explainability, these distinct policies are used to "explain" an agent's options in terms of available trajectory shapes from which a human user may choose. In particular, DNA applies to value function-based policies on Markov decision processes where agents are limited to continuous trajectories. Here, we describe DNA, which uses reward shaping in local, modified Q-learning problems to solve for distinct policies with guaranteed epsilon-optimality. We show that it successfully returns qualitatively different policies that constitute meaningfully different "options" in simulation, including a brief comparison to related approaches in the stochastic optimization field of Quality Diversity.}
}

@InProceedings{assifpk25a,
    title = {Topological State Space Inference for Dynamical Systems},
    author = {K, Mishal Assif P and Baryshnikov, Yuliy},
    pages = {1206--1216},
    abstract = {We present a computational pipe aiming at recovery of the topology of the underlying phase space from observation of an output function along a sample of trajectories of a dynamical system.}
}

@InProceedings{miao25a,
    title = {Opt-ODENet: Neural ODE Controller Design with Differentiable Optimization Layers for Safety and Stability},
    author = {Miao, Keyan and Zhao, Liqun and Wang, Han and Gatsis, Konstantinos and Papachristodoulou, Antonis},
    pages = {1217--1229},
    abstract = {Designing controllers that achieve task objectives while ensuring safety is a key challenge in control systems. This work introduces Opt-ODENet, a Neural ODE framework with a differentiable Quadratic Programming (QP) optimization layer to enforce constraints as hard requirements. Eliminating the reliance on nominal controllers or large datasets, our framework solves the optimal control problem directly using Neural ODEs. Stability and convergence are ensured through Control Lyapunov Functions (CLFs) in the loss function, while Control Barrier Functions (CBFs) embedded in the QP layer enforce real-time safety. By integrating the differentiable QP layer with Neural ODEs, we demonstrate compatibility with the adjoint method for gradient computation, enabling the learning of the CBF class-$\mathcal{K}$ function and control network parameters. Experiments validate its effectiveness in balancing safety and performance.}
}

@InProceedings{li25e,
    title = {NAPI-MPC: Neural Accelerated Physics-Informed MPC for Nonlinear PDE Systems},
    author = {Li, Peilun and Tan, Kaiyuan and Beckers, Thomas},
    pages = {1230--1242},
    abstract = {The control of systems governed by nonlinear partial differential equations (PDEs) can present substantial challenges for traditional linear model predictive control (MPC) approaches. Data-driven MPC has emerged as a solution for dealing with unknown nonlinear dynamics, but issues regarding safety guarantees, accuracy, and data efficiency remain a concern. We propose NAPI-MPC: a novel physics-informed scenario-based MPC approach for control of nonlinear PDE systems with partially unknown dynamics. Unlike other physics-informed learning methods that require extensive knowledge on the governing equations, our NAPI-MPC leverages distributed Port-Hamiltonian systems as a generalized, energy-based representation of the PDE dynamics, in which the Hamiltonian is modeled and learned by a Gaussian process. The Bayesian nature of the Gaussian process enables the drawing of scenario samples that are used in scenario-based predictive control to determine the optimal control action for the PDE system. To ensure applicability in time-sensitive contexts, we leverage a neural network as proxy for the MPC controller, trained offline on states and optimal control actions to enable fast inference for real-time operation.}
}

@InProceedings{maheshwari25a,
    title = {BIGE : Biomechanics-informed GenAI for Exercise Science},
    author = {Maheshwari, Shubh and Mohanty, Anwesh and Cao, Yadi and Razu, Swithin and McCulloch, Andrew and Yu, Rose},
    pages = {1243--1256},
    abstract = {Proper movements enhance mobility, coordination, and muscle activation, which are crucial for performance, injury prevention, and overall fitness. However, traditional simulation tools rely on strong modeling assumptions, are difficult to set up and computationally expensive. On the other hand, generative AI approaches provide efficient alternatives to motion generation. But they often lack physiological relevance and do not incorporate biomechanical constraints, limiting their practical applications in sports and exercise science. To address these limitations, we propose a novel framework, BIGE, that combines bio-mechanically meaningful scoring metrics with generative modeling. BIGE integrates a differentiable surrogate model for muscle activation to reverse optimize the latent space of the generative model, enabling the retrieval of physiologically valid motions through targeted search. Through extensive experiments on squat exercise data, our framework demonstrates superior performance in generating diverse, physically plausible motions while maintaining high fidelity to clinician-defined objectives compared to existing approaches.}
}

@InProceedings{lu25a,
    title = {Safe Learning in the Real World via Adaptive Shielding with Hamilton-Jacobi Reachability},
    author = {Lu, Michael and Gosain, Jashanraj and Sang, Luna and Chen, Mo},
    pages = {1257--1270},
    abstract = {We present a robust shielding framework using Hamilton-Jacobi Reachability that can be combined with any off-policy Reinforcement Learning to enable safer learning. Using an approximate model of a system dynamics, our method can capture the local model mismatch from a safety perspective. This leads to a more conservative safety filter that can adapt to model mismatch. Using a Turtlebot 2, we demonstrate that our method can allow for safe learning in the real-world with minimal human intervention.}
}

@InProceedings{sattar25a,
    title = {Finite Sample Identification of Partially Observed Bilinear Dynamical Systems},
    author = {Sattar, Yahya and Jedra, Yassir and Fazel, Maryam and Dean, Sarah},
    pages = {1271--1285},
    abstract = {We consider the problem of learning a realization of a partially observed bilinear dynamical system (BLDS) from noisy input-output data. Given a single trajectory of input-output samples, we provide an algorithm and a finite time analysis for learning the system's Markov-like parameters, from which a balanced realization of the bilinear system can be obtained. The stability of BLDS depends on the sequence of inputs used to excite the system. Moreover, our identification algorithm regresses the outputs to highly correlated, nonlinear, and heavy-tailed covariates. These properties, unique to partially observed bilinear dynamical systems, pose significant challenges to the analysis of our algorithm for learning the unknown dynamics. We address these challenges and provide high probability error bounds on our identification algorithm under a uniform stability assumption. Our analysis provides insights into system theoretic quantities that affect learning accuracy and sample complexity. Lastly, we perform numerical experiments with synthetic data to reinforce these insights.}
}

@InProceedings{tadipatri25a,
    title = {Nonconvex Linear System Identification with Minimal State Representation},
    author = {Tadipatri, Uday Kiran Reddy and Haeffele, Benjamin D. and Agterberg, Joshua and Ziemann, Ingvar and Vidal, Rene},
    pages = {1286--1299},
    abstract = {Low-order linear System IDentification (SysID) addresses the challenge of estimating the parameters of a linear dynamical system from finite samples of observations and control inputs with minimal state representation. Traditional approaches often utilize Hankel-rank minimization, which relies on convex relaxations that can require numerous, costly singular value decompositions (SVDs) to optimize. In this work, we propose two nonconvex reformulations to tackle low-order SysID (i) Burer-Monterio (BM) factorization of the Hankel matrix for efficient nuclear norm minimization, and (ii) optimizing directly over system parameters for real, diagonalizable systems with an atomic norm style decomposition. These reformulations circumvent the need for repeated heavy SVD computations, significantly improving computational efficiency. Moreover, we prove that optimizing directly over the system parameters yields lower statistical error rates, and lower sample complexities that do not scale linearly with trajectory length like in Hankel-nuclear norm minimization. Additionally, while our proposed formulations are nonconvex, we provide theoretical guarantees of achieving global optimality in polynomial time. Finally, we demonstrate algorithms that solve these nonconvex programs and validate our theoretical claims on synthetic data.}
}

@InProceedings{yang25a,
    title = {Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual Servoing of Soft Continuum Arms},
    author = {Yang, Hsin-Jung and Khosravi, Mahsa and Walt, Benjamin and Krishnan, Girish and Sarkar, Soumik},
    pages = {1300--1312},
    abstract = {Soft continuum arms (SCAs) soft and deformable nature presents challenges in modeling and control due to their infinite degrees of freedom and non-linear behavior. This work introduces a reinforcement learning (RL)-based framework for visual servoing tasks on SCAs with zero-shot sim-to-real transfer capabilities, demonstrated on a single section pneumatic manipulator capable of bending and twisting. The framework decouples kinematics from mechanical properties using an RL kinematic controller for motion planning and a local controller for actuation refinement, leveraging minimal sensing with visual feedback. Trained entirely in simulation, the RL controller achieved a 99.8\% success rate. When deployed on hardware, it achieved a 67\% success rate in zero-shot sim-to-real transfer, demonstrating robustness and adaptability. This approach offers a scalable solution for SCAs in 3D visual servoing, with potential for further refinement and expanded applications.}
}

@InProceedings{rui25a,
    title = {Finite Sample Analysis of Tensor Decomposition for Learning Mixtures of Linear Systems},
    author = {Rui, Maryann and Dahleh, Munther},
    pages = {1313--1325},
    abstract = {We study the problem of learning mixtures of linear dynamical systems (MLDS) from input-output data. The mixture setting allows us to leverage observations from related dynamical systems to improve the estimation of individual models. Building on spectral methods for mixtures of linear regressions, we propose a moment-based estimator that uses tensor decomposition to estimate the impulse response parameters of the mixture models. The estimator improves upon existing tensor decomposition approaches for MLDS by utilizing the entire length of the observed trajectories. We provide sample complexity bounds for estimating MLDS in the presence of noise, in terms of both the number of trajectories $N$ and the trajectory length $T$, and demonstrate the performance of the estimator through simulations.}
}

@InProceedings{rabiee25a,
    title = {Safe Exploration in Reinforcement Learning: Training Backup Control Barrier Functions with Zero Training-Time Safety Violations},
    author = {Rabiee, Pedram and Safari, Amirsaeid},
    pages = {1326--1337},
    abstract = {This paper introduces the Reinforcement Learning Backup Shield (RLBUS), a framework that guarantees safe exploration in reinforcement learning (RL) by incorporating Backup Control Barrier Functions (BCBFs). RLBUS synthesizes an implicit control forward invariant subset of the safe set using multiple backup policies, ensuring safety in the presence of input constraints. While traditional BCBFs often yield conservative control forward invariant sets due to the design of backup controllers, RLBUS addresses this limitation by leveraging model-free RL to train an additional backup policy, enlarging the identified forward invariant subset of the safe set. This approach enables safe exploration of larger regions of the state space with zero safety violations during training. The effectiveness of RLBUS is demonstrated on an inverted pendulum example, where the expanded invariant set facilitates safe exploration over a broader state space, enhancing performance without compromising safety.}
}

@InProceedings{chatzipantazis25a,
    title = {STRiDE: STate-space Riemannian Diffusion for Equivariant Planning},
    author = {Chatzipantazis, Evangelos and Rao, Nishanth and Daniilidis, Kostas},
    pages = {1338--1352},
    abstract = {Fast and reliable motion planning is essential for robots with many degrees of freedom in complex, dynamic environments. Diffusion models offer a promising alternative to classical planners by learning informative trajectory priors. In current imitation-learning paradigms, these models are kept lightweight---lacking encoders---and trained to overfit to a single environment. As a result, adaptation relies solely on diffusion guidance, which fails under large execution-time changes or varying initializations. In addition, current approaches ignore the underlying topology of the state space thus requiring heavy guidance that dominates planning time and reduces efficiency dramatically. We introduce STRiDE, a novel diffusion motion planner that operates directly on the state space manifold and learns equivariant trajectory priors. Our approach eliminates the need for retraining under rotations around the gravity axis and enables faster convergence using Riemannian (rather than ambient) guidance. STRiDE delivers efficient, robust, and generalizable planning, overcoming key limitations of existing approaches.}
}

@InProceedings{hsu25a,
    title = {Safe Cooperative Multi-Agent Reinforcement Learning with Function Approximation},
    author = {Hsu, Hao-Lun and Pajic, Miroslav},
    pages = {1353--1364},
    abstract = {Cooperative multi-agent reinforcement learning (MARL) has shown significant promise in dynamic control environments, where effective communication and tailored exploration strategies facilitate collaboration. However, ensuring safe exploration remains challenging, as even a single unsafe action from any agent can lead to severe consequences. To mitigate this risk, we introduce Scoop-LSVI, a UCB-based cooperative parallel RL framework that achieves low cumulative regret with minimal communication demands while adhering to safety constraints. This framework enables multiple agents to concurrently solve isolated Markov Decision Processes (MDPs) and share information to enhance learning efficiency. Scoop-LSVI attains a regret of $\Tilde{O}(\kappa d^{3/2} H^2 \sqrt{MK})$, where $d$ is the feature dimension, $H$ is the horizon length, $M$ is the number of agents, $K$ is the number of episodes for each agent, and $\kappa$ represents safety constraints. This result aligns with state-of-the-art findings for unsafe cooperative MARL and also matches the regret bounds of UCB-based single-agent RL algorithms ($M = 1$), highlighting the potential of Scoop-LSVI to support safe and efficient learning in cooperative MARL applications.}
}

@InProceedings{krasowski25a,
    title = {Learning Biomolecular Models using Signal Temporal Logic},
    author = {Krasowski, Hanna and Palanques-Tost, Eric and Belta, Calin and Arcak, Murat},
    pages = {1365--1377},
    abstract = {Modeling dynamical biological systems is key for understanding, predicting, and controlling complex biological behaviors. Traditional methods for identifying governing equations, such as ordinary differential equations (ODEs), typically require extensive quantitative data, which is often scarce in biological systems due to experimental limitations. To address this challenge, we introduce an approach that determines biomolecular models from qualitative system behaviors expressed as Signal Temporal Logic (STL) statements, which are naturally suited to translate expert knowledge into computationally tractable specifications. Our method represents the biological network as a graph, where edges represent interactions between species, and uses a genetic algorithm to identify the graph. To infer the parameters of the ODEs modeling the interactions, we propose a gradient-based algorithm. On a numerical example, we evaluate two loss functions using STL robustness and analyze different initialization techniques to improve the convergence of the approach.}
}

@InProceedings{syed25a,
    title = {Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR},
    author = {Syed, Shahbaz P Qadri and Bai, He},
    pages = {1378--1391},
    abstract = {Developing scalable and efficient reinforcement learning algorithms for cooperative multi-agent control has received significant attention over the past years. Existing literature has proposed inexact decompositions of local Q-functions based on empirical information structures between the agents. In this paper, we exploit inter-agent coupling information and propose a systematic approach to exactly decompose the local Q-function of each agent. We develop an approximate least square policy iteration algorithm based on the proposed decomposition and identify two architectures to learn the local Q-function for each agent. We establish that the worst-case sample complexity of the decomposition is equal to the centralized case and derive necessary and sufficient graphical conditions on the inter-agent couplings to achieve better sample efficiency. We demonstrate the improved sample efficiency and computational efficiency on numerical examples.}
}

@InProceedings{xie25a,
    title = {Morphological-Symmetry-Equivariant Heterogeneous Graph Neural Network for Robotic Dynamics Learning},
    author = {Xie, Fengze and Wei, Sizhe and Song, Yue and Yue, Yisong and Gan, Lu},
    pages = {1392--1405},
    abstract = {We propose MS-HGNN, a Morphological-Symmetry-Equivariant Heterogeneous Graph Neural Network for robotic dynamics learning, which integrates robotic kinematic structures and morphological symmetries into a unified graph network. By embedding these structural priors as inductive biases, MS-HGNN ensures high generalizability, sample and model efficiency. This architecture is versatile and broadly applicable to various multi-body dynamic systems and dynamics learning tasks. We prove the morphological-symmetry-equivariant property of MS-HGNN and demonstrate its effectiveness across multiple quadruped robot dynamics learning problems using real-world and simulated data. Our code is available at https://github.com/lunarlab-gatech/MorphSym-HGNN/.}
}

@InProceedings{lee25a,
    title = {Learning Collective Dynamics of Multi-Agent Systems using Event-based Vision},
    author = {Lee, Minah and Kamal, Uday and Mukhopadhyay, Saibal},
    pages = {1406--1418},
    abstract = {This paper proposes a novel problem: vision-based perception to learn and predict the collective dynamics of multi-agent systems, specifically focusing on interaction strength and convergence time. Multi-agent systems are defined as collections of more than ten interacting agents that exhibit complex group behaviors. Unlike prior studies that assume knowledge of agent states, we focus on deep learning models to directly predict collective dynamics from visual data, captured as frames or events. Due to the lack of relevant datasets, we create a simulated dataset using a state-of-the-art flocking simulator, coupled with a vision-to-event conversion framework. We empirically demonstrate the effectiveness of event-based representation over traditional frame-based methods in predicting these collective behaviors. Based on our analysis, we present event-based vision for Multi-Agent dynamic Prediction (evMAP), a deep learning architecture designed for real-time, accurate understanding of interaction strength and collective behavior emergence in multi-agent systems.}
}

@InProceedings{soltanian25a,
    title = {PACE: A Framework for Learning and Control in Linear Incomplete-Information Differential Games},
    author = {Soltanian, Seyed Yousef and Zhang, Wenlong},
    pages = {1419--1433},
    abstract = {In this paper, we address the problem of a two-player linear quadratic differential game with incomplete information, a scenario commonly encountered in multi-agent control, human-robot interaction (HRI), and approximation methods to solve general-sum differential games. While solutions to such linear differential games are typically obtained through coupled Riccati equations, the complexity increases when agents have incomplete information, particularly when neither is aware of the other's cost function. To tackle this challenge, we propose a model-based Peer-Aware Cost Estimation (PACE) framework for learning the cost parameters of the other agent. In PACE, each agent treats its peer as a learning agent rather than a stationary optimal agent, models their learning dynamics, and leverages this dynamic to infer the cost function parameters of the other agent. This approach enables agents to infer each other's objective function in real time based solely on their previous state observations and dynamically adapt their control policies. Furthermore, we provide a theoretical guarantee for the convergence of parameter estimation and the stability of system states in PACE. Additionally, using numerical studies, we demonstrate how modeling the learning dynamics of the other agent benefits PACE, compared to approaches that approximate the other agent as having complete information, particularly in terms of stability and convergence speed.}
}

@InProceedings{tebbe25a,
    title = {Physics-informed Gaussian Processes as Linear Model Predictive Controller},
    author = {Tebbe, J{\"o}rn and Besginow, Andreas and Lange-Hegermann, Markus},
    pages = {1434--1446},
    abstract = {We introduce a novel algorithm for controlling linear time invariant systems in a tracking problem. The controller is based on a Gaussian Process (GP) whose realizations satisfy a system of linear ordinary differential equations with constant coefficients. Control inputs for tracking are determined by conditioning the prior GP on the setpoints, i.e. control as inference. The resulting Model Predictive Control scheme incorporates pointwise soft constraints by introducing virtual setpoints to the posterior Gaussian process. We show theoretically that our controller satisfies open-loop stability for the optimal control problem by leveraging general results from Bayesian inference and demonstrate this result in a numerical example.}
}

@InProceedings{li25d,
    title = {Neural Contraction Metrics with Formal Guarantees for Discrete-Time Nonlinear Dynamical Systems},
    author = {Li, Haoyu and Zhong, Xiangru and Hu, Bin and Zhang, Huan},
    pages = {1447--1459},
    abstract = {Contraction metrics are crucial in control theory because they provide a powerful framework for analyzing stability, robustness, and convergence of various dynamical systems. However, identifying these metrics for complex nonlinear systems remains an open challenge due to the lack of scalable and effective tools. This paper explores the approach of learning verifiable contraction metrics parametrized as neural networks (NNs) for discrete-time nonlinear dynamical systems. While prior works on formal verification of contraction metrics for general nonlinear systems have focused on convex optimization methods (e.g. linear matrix inequalities, etc) under the assumption of continuously differentiable dynamics, the growing prevalence of NN-based controllers, often utilizing ReLU activations, introduces challenges due to the non-smooth nature of the resulting closed-loop dynamics. To bridge this gap, we establish a new sufficient condition for establishing formal neural contraction metrics for general discrete-time nonlinear systems assuming only the continuity of the dynamics. We show that from a computational perspective, our sufficient condition can be efficiently verified using the state-of-the-art neural network verifier alpha-beta-CROWN, which scales up non-convex neural network verification via novel integration of symbolic linear bound propagation and branch-and-bound. Built upon our analysis tool, we further develop a learning method for synthesizing neural contraction metrics from sampled data. Finally, our approach is validated through the successful synthesis and verification of NN contraction metrics for various nonlinear examples.}
}

@InProceedings{monir25a,
    title = {Robust Control of Uncertain Switched Affine Systems via Scenario Optimization},
    author = {Monir, Negar and Sadabadi, Mahdieh S. and Soudjani, Sadegh},
    pages = {1460--1471},
    abstract = {Switched affine systems are often used to model and control complex dynamical systems that operate in multiple modes. However, uncertainties in the system matrices can challenge their stability and performance. This paper introduces a new approach for designing switching control laws for uncertain switched affine systems using data-driven scenario optimization. Instead of relaxing invariant sets, our method creates smaller invariant sets with quadratic Lyapunov functions through scenario-based optimization, effectively reducing chattering effects and regulation error. The framework ensures robustness against parameter uncertainties while improving accuracy. We validate our method with applications in multi-objective interval Markov decision processes and power electronic converters, demonstrating its versatility and effectiveness.}
}

@InProceedings{an25a,
    title = {Disentangling Uncertainties by Learning Compressed Data Representation},
    author = {An, Zhiyu and Hou, Zhibo and Du, Wan},
    pages = {1472--1483},
    abstract = {We study aleatoric and epistemic uncertainty estimation in a learned regressive system dynamics model. Disentangling aleatoric uncertainty (the inherent randomness of the system) from epistemic uncertainty (the lack of data) is crucial for downstream tasks such as risk-aware control and reinforcement learning, efficient exploration, and robust policy transfer. While existing approaches like Gaussian Processes, Bayesian networks, and model ensembles are widely adopted, they suffer from either high computational complexity or inaccurate uncertainty estimation. To address these limitations, we propose the Compressed Data Representation Model (CDRM), a framework that learns a neural network encoding of the data distribution and enables direct sampling from the output distribution. Our approach incorporates a novel inference procedure based on Langevin dynamics sampling, allowing CDRM to predict arbitrary output distributions rather than being constrained to a Gaussian prior. Theoretical analysis provides the conditions where CDRM achieves better memory and computational complexity compared to bin-based compression methods. Empirical evaluations show that CDRM demonstrates a superior capability to identify aleatoric and epistemic uncertainties separately, achieving AUROCs of 0.8876 and 0.9981 on a single test set containing a mixture of both uncertainties. Qualitative results further show that CDRM's capability extends to datasets with multimodal output distributions, a challenging scenario where existing methods consistently fail. Code and supplementary materials are available at \url{https://github.com/ryeii/CDRM}.}
}

@InProceedings{puthumanaillam25a,
    title = {TAB-Fields: A Maximum Entropy Framework for Mission-Aware Adversarial Planning},
    author = {Puthumanaillam, Gokul and Song, Jae Hyuk and Yesmagambet, Nurzhan and Park, Shinkyu and Ornik, Melkior},
    pages = {1484--1497},
    abstract = {Autonomous agents operating in adversarial scenarios face a fundamental challenge: while they may know their adversaries' high-level objectives, such as reaching specific destinations within time constraints, the exact policies these adversaries will employ remain unknown. Traditional approaches address this challenge by treating the adversary's state as a partially observable element, leading to a formulation as a Partially Observable Markov Decision Process (POMDP). However, the induced belief-space dynamics in a POMDP require knowledge of the system's transition dynamics, which, in this case, depend on the adversary's unknown policy. Our key observation is that while an adversary's exact policy is unknown, their behavior is necessarily constrained by their mission objectives and the physical environment, allowing us to characterize the space of possible behaviors without assuming specific policies. In this paper, we develop Task-Aware Behavior Fields (TAB-Fields), a representation that captures adversary state distributions over time by computing the most unbiased probability distribution consistent with known constraints. We construct TAB-Fields by solving a constrained optimization problem that minimizes additional assumptions about adversary behavior beyond mission and environmental requirements. We integrate TAB-Fields with standard planning algorithms by introducing TAB-conditioned POMCP, an adaptation of Partially Observable Monte Carlo Planning. Through experiments in simulation with underwater robots and hardware implementations with ground robots, we demonstrate that our approach achieves superior performance compared to baselines that either assume specific adversary policies or neglect mission constraints altogether. Evaluation videos and code are available at https://tab-fields.github.io.}
}

@InProceedings{zhong25a,
    title = {Bridging Adaptivity and Safety: Learning Agile Collision-Free Locomotion Across Varied Physics},
    author = {Zhong, Yichao and Zhang, Chong and He, Tairan and Shi, Guanya},
    pages = {1498--1511},
    abstract = {Real-world legged locomotion systems often need to reconcile agility and safety for different scenarios. Moreover, the underlying dynamics are often unknown and time-variant (e.g., payload, friction). In this paper, we introduce BAS (Bridging Adaptivity and Safety), which builds upon the pipeline of prior work Agile But Safe (ABS) and is designed to provide adaptive safety even in dynamic environments with uncertainties. BAS involves an agile policy to avoid obstacles rapidly and a recovery policy to prevent collisions, a physical parameter estimator that is concurrently trained with agile policy, and a learned control-theoretic RA (reach-avoid) value network that governs the policy switch. Also, the agile policy and RA network are both conditioned on physical parameters to make them adaptive. To mitigate the distribution shift issue, we further introduce an on-policy fine-tuning phase for the estimator to enhance its robustness and accuracy. The simulation results show that BAS achieves 50\% better safety in dynamic environments while maintaining a higher speed on average. In real-world experiments, BAS shows its capability in complex environments with unknown physics (e.g., slippery floors with unknown frictions, carrying up unknown payloads up to 8kg), while baselines lack adaptivity, leading to collisions or degraded agility. As a result, BAS achieves a 19.8\% increase in speed and gets a 2.36 times lower collision rate than ABS in the real world. More detailed videos of the real-world performance of BAS are listed on our website: https://adaptive-safe-locomotion.github.io}
}

@InProceedings{canyakmaz25a,
    title = {Learning and steering game dynamics towards desirable outcomes},
    author = {Canyakmaz, Ilayda and Sakos, Iosif and Lin, Wayne and Varvitsiotis, Antonios and Piliouras, Georgios},
    pages = {1512--1524},
    abstract = {Game dynamics, which describe how agents' strategies evolve over time based on past interactions, can exhibit a variety of undesirable behaviours including convergence to suboptimal equilibria, cycling, and chaos. While central planners can employ incentives to mitigate such behaviors and steer game dynamics towards desirable outcomes, the effectiveness of such interventions critically relies on accurately predicting agents' responses to these incentives---a task made particularly challenging when the underlying dynamics are unknown and observations are limited. To address this challenge, this work introduces the Side Information Assisted Regression with Model Predictive Control (SIAR-MPC) framework. We extend the recently introduced SIAR method to incorporate the effect of control, enabling it to utilize side-information constraints inherent to game-theoretic applications to model agents' responses to incentives from scarce data. MPC then leverages this model to implement dynamic incentive adjustments. Our experiments demonstrate the effectiveness of SIAR-MPC in guiding systems towards socially optimal equilibria, stabilizing chaotic and cycling behaviors. Notably, it achieves these results in data-scarce settings of few learning samples, where well-known system identification methods paired with MPC show less effective results.}
}

@InProceedings{yu25a,
    title = {Interaction-Aware Parameter Privacy-Preserving Data Sharing in Coupled Systems via Particle Filter Reinforcement Learning},
    author = {Yu, Haokun and Zhou, Jingyuan and Yang, Kaidi},
    pages = {1525--1536},
    abstract = {This paper addresses the problem of parameter privacy-preserving data sharing in coupled systems, where a data provider shares data with a data user but wants to protect its sensitive parameters. The shared data affects not only the data user's decision-making but also the data provider's operations through system interactions. To trade off control performance and privacy, we propose an interaction-aware privacy-preserving data sharing approach. Our approach generates distorted data by minimizing a combination of (i) mutual information that quantifies the privacy leakage of sensitive parameters and (ii) the impact of distorted data on the data provider's control performance, considering the interactions between stakeholders. The optimization problem is formulated into a Bellman equation and solved by a particle filter reinforcement learning (RL)-based approach. Compared to existing RL-based methods, our formulation significantly reduces history dependency and efficiently handles scenarios with continuous state space. The proposed method is validated in a mixed-autonomy platoon scenario, where it successfully protects sensitive driving behavior parameters of human-driven vehicles (HDVs) against inference attacks while maintaining negligible impact on fuel efficiency.}
}

@InProceedings{gracia25a,
    title = {Temporal Logic Control for Nonlinear Stochastic Systems Under Unknown Disturbances},
    author = {Gracia, Ibon and Laurenti, Luca and Jr, Manuel Mazo and Abate, Alessandro and Lahijanian, Morteza},
    pages = {1537--1549},
    abstract = {In this paper, we present a novel framework to synthesize robust strategies for discrete-time non-linear systems with and random disturbances that are unknown and non-additive, against temporal logic specifications. The proposed framework is data-driven and abstraction-based: leveraging observations of the system, our approach learns a high-confidence abstraction of the system in the form of an uncertain Markov decision process (UMDP). The uncertainty in the resulting UMDP is used to formally account for both the error in abstracting the system and for the uncertainty coming from the data. Critically, we show that for any given state-action pair in the resulting UMDP, the uncertainty in the transition probabilities can be represented as a convex polytope obtained by a 2-layer state discretization and concentration inequalities. This allows us to obtain tighter uncertainty estimates compared to existing approaches, and guarantees efficiency, as we tailor a synthesis algorithm exploiting the structure of this UMDP. We empirically validate our approach on several case studies, showing substantially improved empirical performance compared to the state-of-the-art.}
}

@InProceedings{nazeri25a,
    title = {Data-Driven Yet Formal Policy Synthesis for Stochastic Nonlinear Dynamical Systems},
    author = {Nazeri, Mahdi and Badings, Thom and Soudjani, Sadegh and Abate, Alessandro},
    pages = {1550--1564},
    abstract = {The automated synthesis of control policies for stochastic dynamical systems presents significant challenges. A standard approach is to construct a finite-state abstraction of the continuous system, typically represented as a Markov decision process (MDP). However, generating abstractions is challenging when (1) the system's dynamics are nonlinear, and/or (2) we do not have complete knowledge of the dynamics. In this work, we introduce a novel data-driven abstraction technique for nonlinear Lipschitz continuous dynamical systems with additive stochastic noise that addresses both of these issues. As a key step, we use samples of the dynamics to learn the enabled actions and transition probabilities of the abstraction. We represent abstractions as MDPs with intervals of transition probabilities, known as interval MDPs (IMDPs). These abstractions enable the synthesis of policies for the concrete nonlinear system, with probably approximately correct (PAC) guarantees on the probability of satisfying a specified control objective. Our numerical experiments illustrate the effectiveness and robustness of our approach in achieving reliable control under uncertainty.}
}

@InProceedings{abou-taleb25a,
    title = {Koopman Based Trajectory Optimization with Mixed Boundaries},
    author = {Abou-Taleb, Mohamed and Raff, Maximilian and Fla{\ss}kamp, Kathrin and Remy, C. David},
    pages = {1565--1577},
    abstract = {Trajectory optimization is a widely used tool in the design and control of dynamical systems. Typically, not only nonlinear dynamics, but also couplings of the initial and final condition through implicit boundary constraints render the optimization problem non-convex. This paper investigates how the Koopman operator framework can be utilized to solve trajectory optimization problems in a (partially) convex fashion. While the Koopman operator has already been successfully employed in model predictive control, the challenge of addressing mixed boundary constraints within the Koopman framework has remained an open question. We first address this issue by explaining why a complete convexification of the problem is not possible. Secondly, we propose a method where we transform the trajectory optimization problem into a bilevel problem in which we are then able to convexify the high-dimensional lower-level problem. This separation yields a low-dimensional upper-level problem, which could be exploited in global optimization algorithms. Lastly, we demonstrate the effectiveness of the method on two example systems: the mathematical pendulum and the compass-gait walker.}
}


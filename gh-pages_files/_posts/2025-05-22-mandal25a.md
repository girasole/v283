---
title: A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks
abstract: 'Knowledge distillation, where a small student model learns from a pre-trained
  large teacher model, has achieved substantial empirical success since the seminal
  work of (Hinton et al., 2015). Despite prior theoretical studies exploring the benefits
  of knowledge distillation, an important question remains unanswered: why does soft-label
  training from the teacher require significantly fewer neurons than directly training
  a small neural network with hard labels? To address this, we first present motivating
  experimental results using simple neural network models on a binary classification
  problem. These results demonstrate that soft-label training consistently outperforms
  hard-label training in accuracy, with the performance gap becoming more pronounced
  as the dataset becomes increasingly difficult to classify. We then substantiate
  these observations with a theoretical contribution based on two-layer neural network
  models. Specifically, we show that soft-label training using gradient descent requires
  only \(O\left(\frac{1}{\gamma^2 \epsilon}\right)\){neurons} to achieve a classification
  loss averaged over epochs smaller than some \(\epsilon > 0\), where \(\gamma\){is}
  the separation margin of the limiting kernel. In contrast, hard-label training requires
  \(O\left(\frac{1}{\gamma^4} \cdot \ln\left(\frac{1}{\epsilon}\right)\right)\){neurons},
  as derived from an adapted version of the gradient descent analysis in (Ji and Telgarsky,
  2020). This implies that when \(\gamma \leq \epsilon\), i.e., when the dataset is
  challenging to classify, the neuron requirement for soft-label training can be significantly
  lower than that for hard-label training. Finally, we present experimental results
  on deep neural networks, further validating these theoretical findings.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mandal25a
month: 0
tex_title: A Theoretical Analysis of Soft-Label vs Hard-Label Training in Neural Networks
firstpage: 1078
lastpage: 1089
page: 1078-1089
order: 1078
cycles: false
bibtex_author: Mandal, Saptarshi and Lin, Xiaojun and Srikant, Rayadurgam
author:
- given: Saptarshi
  family: Mandal
- given: Xiaojun
  family: Lin
- given: Rayadurgam
  family: Srikant
date: 2025-05-22
address:
container-title: Proceedings of the 7th Annual Learning for Dynamics \& Control Conference
volume: '283'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 22
pdf: https://raw.githubusercontent.com/mlresearch/v283/main/assets/mandal25a/mandal25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

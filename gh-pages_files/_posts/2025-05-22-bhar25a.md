---
title: Scalability Enhancement and Data-Heterogeneity Awareness in Gradient Tracking
  based Decentralized Bayesian Learning
abstract: This paper proposes a Gradient Tracking Decentralized Unadjusted Langevin
  Algorithm (GT-DULA) to perform Bayesian learning via MCMC sampling. GT-DULA enhances
  the scalability of the process when compared with the conventional DULA as it reduces
  the dependence of the convergence bias on the network size by an order of magnitude
  for constant gradient step size. GT-DULA uses an estimate of the global gradient
  as a substitute for local gradients which is shared among neighbors in the network.
  Our theoretical analysis shows that the proposed GT-DULA successfully tracks the
  global gradient within a certain neighborhood, which leads to a two-fold benefit.
  First, the optimal mixing of the gradient estimates leads to a lower bias in convergence.
  Second, the successful tracking of the global gradient implies robustness towards
  data heterogeneity which is a major concern in decentralized learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bhar25a
month: 0
tex_title: Scalability Enhancement and Data-Heterogeneity Awareness in Gradient Tracking
  based Decentralized Bayesian Learning
firstpage: 591
lastpage: 605
page: 591-605
order: 591
cycles: false
bibtex_author: Bhar, Kinjal and Bai, He and George, Jemin and Busart, Carl
author:
- given: Kinjal
  family: Bhar
- given: He
  family: Bai
- given: Jemin
  family: George
- given: Carl
  family: Busart
date: 2025-05-22
address:
container-title: Proceedings of the 7th Annual Learning for Dynamics \& Control Conference
volume: '283'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 22
pdf: https://raw.githubusercontent.com/mlresearch/v283/main/assets/bhar25a/bhar25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

---
title: Anytime Safe Reinforcement Learning
abstract: This paper considers the problem of solving constrained reinforcement learning
  problems with anytime guarantees, meaning that the algorithmic solution returns
  a safe policy regardless of when it is terminated. Drawing inspiration from anytime
  constrained optimization, we introduce Reinforcement Learning-based Safe Gradient
  Flow (RL-SGF), an on-policy algorithm which employs estimates of the value functions
  and their respective gradients associated with the objective and safety constraints
  for the current policy, and updates the policy parameters by solving a convex quadratically
  constrained quadratic program. We show that if the estimates are computed with a
  sufficiently large number of episodes (for which we provide an explicit bound),
  safe policies are updated to safe policies with a probability higher than a prescribed
  tolerance. We also show that iterates asymptotically converge to a neighborhood
  of a KKT point, whose size can be arbitrarily reduced by refining the estimates
  of the value function and their gradients. We illustrate the performance of RL-SGF
  in a navigation example.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mestres25a
month: 0
tex_title: Anytime Safe Reinforcement Learning
firstpage: 221
lastpage: 232
page: 221-232
order: 221
cycles: false
bibtex_author: Mestres, Pol and Marzabal, Arnau and Cortes, Jorge
author:
- given: Pol
  family: Mestres
- given: Arnau
  family: Marzabal
- given: Jorge
  family: Cortes
date: 2025-05-22
address:
container-title: Proceedings of the 7th Annual Learning for Dynamics \& Control Conference
volume: '283'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 22
pdf: https://raw.githubusercontent.com/mlresearch/v283/main/assets/mestres25a/mestres25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---

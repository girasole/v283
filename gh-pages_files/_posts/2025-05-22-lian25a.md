---
title: Robust Inverse Reinforcement Learning Control with Unknown States
abstract: This paper designs a robust inverse reinforcement learning (IRL) algorithm
  that observes the expert’s inputs and outputs to reconstruct the underlying cost
  function weights and optimal control policy for optimal discrete-time (DT) output
  feedback (OPFB) control systems while admitting disturbances and unknown states.
  The expert system is captured by a zero-sum game where its OPFB controller minimizes
  a cost function while robustly mitigating the effect of the worst disturbance, achieving
  a prescribed attenuation level. The inputs and outputs of the expert can be observed,
  but not the states. To enable the learner to replicate the behavior of the expert,
  we first develop a model-based IRL algorithm and subsequently design an equivalent
  model-free, data-driven version. This latter infers the quadratic cost function
  weights that can yield the expert’s static OPFB control policy, using output and
  input data of both the expert and learner. The convergence of the proposed algorithms
  is rigorously validated through theoretical analysis and numerical experiments.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lian25a
month: 0
tex_title: Robust Inverse Reinforcement Learning Control with Unknown States
firstpage: 750
lastpage: 762
page: 750-762
order: 750
cycles: false
bibtex_author: Lian, Bosen and Xue, Wenqian and Nguyen, Nhan
author:
- given: Bosen
  family: Lian
- given: Wenqian
  family: Xue
- given: Nhan
  family: Nguyen
date: 2025-05-22
address:
container-title: Proceedings of the 7th Annual Learning for Dynamics \& Control Conference
volume: '283'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 5
  - 22
pdf: https://raw.githubusercontent.com/mlresearch/v283/main/assets/lian25a/lian25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
